{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ddb2a1e-4011-49bc-b441-7a3e7e0b9c33",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37027fd6-81f5-4ca2-af58-ba1720157304",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "import random\n",
    "from enum import Enum\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.utils.data as data\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d7b2e29-6103-487f-b528-c0fa2c2d88c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U retinaface_pytorch > /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "1869c701-f376-4778-b798-a1fd9524ea9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n",
      "/opt/conda/lib/python3.8/site-packages/torch/hub.py:651: UserWarning: Falling back to the old format < 1.6. This support will be deprecated in favor of default zipfile format introduced in 1.6. Please redo torch.save() to save it in the new zipfile format.\n",
      "  warnings.warn('Falling back to the old format < 1.6. This support will be '\n"
     ]
    }
   ],
   "source": [
    "from retinaface.pre_trained_models import get_model as get_detector\n",
    "\n",
    "face_detector = get_detector(\"resnet50_2020-07-20\", max_size=512)\n",
    "face_detector.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4048fe9f-6c9c-43f9-99f2-a5f4d80de370",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskLabels(int, Enum):\n",
    "    MASK = 0\n",
    "    INCORRECT = 1\n",
    "    NORMAL = 2\n",
    "\n",
    "\n",
    "class GenderLabels(int, Enum):\n",
    "    MALE = 0\n",
    "    FEMALE = 1\n",
    "\n",
    "    @classmethod\n",
    "    def from_str(cls, value: str) -> int:\n",
    "        value = value.lower()\n",
    "        if value == \"male\":\n",
    "            return cls.MALE\n",
    "        elif value == \"female\":\n",
    "            return cls.FEMALE\n",
    "        else:\n",
    "            raise ValueError(f\"Gender value should be either 'male' or 'female', {value}\")\n",
    "\n",
    "\n",
    "class AgeLabels(int, Enum):\n",
    "    YOUNG = 0\n",
    "    MIDDLE = 1\n",
    "    OLD = 2\n",
    "\n",
    "    @classmethod\n",
    "    def from_number(cls, value: str) -> int:\n",
    "        try:\n",
    "            value = int(value)\n",
    "        except Exception:\n",
    "            raise ValueError(f\"Age value should be numeric, {value}\")\n",
    "\n",
    "        if value < 30:\n",
    "            return cls.YOUNG\n",
    "        elif value < 60:\n",
    "            return cls.MIDDLE\n",
    "        else:\n",
    "            return cls.OLD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7fc3b92-0145-473a-ada9-56f98da7aa0d",
   "metadata": {},
   "source": [
    "# face bounding boxes for training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "fa7c7395-7f6a-4ca0-bfbb-e3e54614f896",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_img_dir = './input/data/train/images'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "5788a7ea-7c5c-490f-ab0c-c9ce38141a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskBaseDataset(data.Dataset):\n",
    "    num_classes = 3 * 2 * 3\n",
    "\n",
    "    _file_names = {\n",
    "        \"mask1\": MaskLabels.MASK,\n",
    "        \"mask2\": MaskLabels.MASK,\n",
    "        \"mask3\": MaskLabels.MASK,\n",
    "        \"mask4\": MaskLabels.MASK,\n",
    "        \"mask5\": MaskLabels.MASK,\n",
    "        \"incorrect_mask\": MaskLabels.INCORRECT,\n",
    "        \"normal\": MaskLabels.NORMAL\n",
    "    }\n",
    "\n",
    "    image_paths = []\n",
    "    mask_labels = []\n",
    "    gender_labels = []\n",
    "    age_labels = []\n",
    "\n",
    "    def __init__(self, img_dir):\n",
    "        self.img_dir = img_dir\n",
    "\n",
    "        self.setup()\n",
    "        \n",
    "    def setup(self):\n",
    "        profiles = os.listdir(self.img_dir)\n",
    "        for profile in profiles:\n",
    "            if profile.startswith(\".\"):  # \".\" 로 시작하는 파일은 무시합니다\n",
    "                continue\n",
    "\n",
    "            img_folder = os.path.join(self.img_dir, profile)\n",
    "            for file_name in os.listdir(img_folder):\n",
    "                _file_name, ext = os.path.splitext(file_name)\n",
    "                if _file_name not in self._file_names:  # \".\" 로 시작하는 파일 및 invalid 한 파일들은 무시합니다\n",
    "                    continue\n",
    "\n",
    "                img_path = os.path.join(self.img_dir, profile, file_name)\n",
    "                mask_label = self._file_names[_file_name]\n",
    "\n",
    "                id, gender, race, age = profile.split(\"_\")\n",
    "                gender_label = GenderLabels.from_str(gender)\n",
    "                age_label = AgeLabels.from_number(age)\n",
    "\n",
    "                self.image_paths.append(img_path)\n",
    "                self.mask_labels.append(mask_label)\n",
    "                self.gender_labels.append(gender_label)\n",
    "                self.age_labels.append(age_label)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # 이미지를 불러옵니다.\n",
    "        image_path = self.image_paths[index]\n",
    "        image = Image.open(image_path)\n",
    "        \n",
    "        # 레이블을 불러옵니다.\n",
    "        mask_label = self.mask_labels[index]\n",
    "        gender_label = self.gender_labels[index]\n",
    "        age_label = self.age_labels[index]\n",
    "        multi_class_label = mask_label * 6 + gender_label * 3 + age_label\n",
    "        \n",
    "        return np.array(image), multi_class_label, image_path\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "094421ed-5c92-4993-b22d-d15100fede00",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MaskBaseDataset(img_dir=train_img_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "79dde1a9-55e4-40d4-8e2c-9750dec69019",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/torch/utils/data/dataset.py:342: UserWarning: Length of split at index 1 is 0. This might result in an empty dataset.\n",
      "  warnings.warn(f\"Length of split at index {i} is 0. \"\n"
     ]
    }
   ],
   "source": [
    "total_dataset, _ = data.random_split(dataset, [1, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "c677ea73-7bb4-48dc-bed2-30068c44f6d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_loader = data.DataLoader(\n",
    "    total_dataset,\n",
    "    batch_size=64,\n",
    "    num_workers=0,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "688a4470-36ce-4648-8446-6cfe6dccdedc",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_paths = []\n",
    "x_mins = []\n",
    "y_mins = []\n",
    "x_maxs = []\n",
    "y_maxs = []\n",
    "\n",
    "for batch in total_loader:\n",
    "    inputs, labels, img_path = batch\n",
    "    \n",
    "    for i, img in enumerate(inputs):\n",
    "        img_paths.append(img_path[i])\n",
    "        annotations = face_detector.predict_jsons(np.array(img))\n",
    "        \n",
    "        try:\n",
    "            x_min, y_min, x_max, y_max = annotations[0][\"bbox\"]\n",
    "            x_mins.append(x_min)\n",
    "            y_mins.append(y_min)\n",
    "            x_maxs.append(x_max)\n",
    "            y_maxs.append(y_max)\n",
    "        except:\n",
    "            x_mins.append(0)\n",
    "            y_mins.append(0)\n",
    "            x_maxs.append(0)\n",
    "            y_maxs.append(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a9ce49c-84ea-40b2-845b-1c57b88bd28e",
   "metadata": {},
   "source": [
    "## test visulization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "cf14f4df-f71b-4386-9594-11dc7b0e3fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For test\n",
    "x_min, y_min, x_max, y_max = x_mins[0], y_mins[0], x_maxs[0], y_maxs[0]\n",
    "test=np.array(Image.open(img_paths[0]))[int(y_min):int(y_max), int(x_min):int(x_max)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa62517a-e5c3-4dd2-b278-b49883f104d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (5,5))\n",
    "plt.imshow(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc0e620f-320c-4178-a8c5-ab8b7d8a2d0f",
   "metadata": {},
   "source": [
    "## save as csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "23813d68-efd5-401f-82fb-f7a656ad89c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'img_paths':img_paths, 'x_min':x_mins, 'y_min':y_mins, 'x_max':x_maxs, 'y_max':y_maxs})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "7ed3c362-532e-4192-af94-8e0fe7c0e928",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('./bounding_box.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd9ab1d-3858-4cf1-91f5-aaaf053b05f2",
   "metadata": {},
   "source": [
    "## Visualizing Data Augmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "id": "b1bbe82a-3cff-42de-a6c2-d71e4b67a738",
   "metadata": {},
   "outputs": [],
   "source": [
    "bounding_box = pd.read_csv('./bounding_box.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "id": "b9223a4e-e506-4a81-a157-9ed2c337885c",
   "metadata": {},
   "outputs": [],
   "source": [
    "bounding_box.set_index('img_paths', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a494a9bf-c95e-466f-9762-e83429e78d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "bounding_box.loc['./input/data/train/images/001060_female_Asian_25/mask2.jpg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "id": "9f3cad61-449d-4f48-b0b7-e85413c080c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskBaseDataset(data.Dataset):\n",
    "    num_classes = 3 * 2 * 3\n",
    "\n",
    "    _file_names = {\n",
    "        \"mask1\": MaskLabels.MASK,\n",
    "        \"mask2\": MaskLabels.MASK,\n",
    "        \"mask3\": MaskLabels.MASK,\n",
    "        \"mask4\": MaskLabels.MASK,\n",
    "        \"mask5\": MaskLabels.MASK,\n",
    "        \"incorrect_mask\": MaskLabels.INCORRECT,\n",
    "        \"normal\": MaskLabels.NORMAL\n",
    "    }\n",
    "\n",
    "    image_paths = []\n",
    "    mask_labels = []\n",
    "    gender_labels = []\n",
    "    age_labels = []\n",
    "\n",
    "    def __init__(self, img_dir, transform=None):\n",
    "        \"\"\"\n",
    "        MaskBaseDataset을 initialize 합니다.\n",
    "\n",
    "        Args:\n",
    "            img_dir: 학습 이미지 폴더의 root directory 입니다.\n",
    "            transform: Augmentation을 하는 함수입니다.\n",
    "        \"\"\"\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "\n",
    "        self.setup()\n",
    "        \n",
    "    def set_transform(self, transform):\n",
    "        \"\"\"\n",
    "        transform 함수를 설정하는 함수입니다.\n",
    "        \"\"\"\n",
    "        self.transform = transform\n",
    "        \n",
    "    def setup(self):\n",
    "        \"\"\"\n",
    "        image의 경로와 각 이미지들의 label을 계산하여 저장해두는 함수입니다.\n",
    "        \"\"\"\n",
    "        profiles = os.listdir(self.img_dir)\n",
    "        for profile in profiles:\n",
    "            if profile.startswith(\".\"):  # \".\" 로 시작하는 파일은 무시합니다\n",
    "                continue\n",
    "\n",
    "            img_folder = os.path.join(self.img_dir, profile)\n",
    "            for file_name in os.listdir(img_folder):\n",
    "                _file_name, ext = os.path.splitext(file_name)\n",
    "                if _file_name not in self._file_names:  # \".\" 로 시작하는 파일 및 invalid 한 파일들은 무시합니다\n",
    "                    continue\n",
    "\n",
    "                img_path = os.path.join(self.img_dir, profile, file_name)  # (resized_data, 000004_male_Asian_54, mask1.jpg)\n",
    "                mask_label = self._file_names[_file_name]\n",
    "\n",
    "                id, gender, race, age = profile.split(\"_\")\n",
    "                gender_label = GenderLabels.from_str(gender)\n",
    "                age_label = AgeLabels.from_number(age)\n",
    "\n",
    "                self.image_paths.append(img_path)\n",
    "                self.mask_labels.append(mask_label)\n",
    "                self.gender_labels.append(gender_label)\n",
    "                self.age_labels.append(age_label)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        데이터를 불러오는 함수입니다. \n",
    "        데이터셋 class에 데이터 정보가 저장되어 있고, index를 통해 해당 위치에 있는 데이터 정보를 불러옵니다.\n",
    "        -\n",
    "        Args:\n",
    "            index: 불러올 데이터의 인덱스값입니다.\n",
    "        \"\"\"\n",
    "        # 이미지를 불러옵니다.\n",
    "        image_path = self.image_paths[index]\n",
    "        image = Image.open(image_path)\n",
    "        image = np.array(image)\n",
    "        \n",
    "        if sum(bounding_box.iloc[index,1:]):\n",
    "            x_min, y_min, x_max, y_max = bounding_box.loc[image_path]\n",
    "            image = image[int(y_min):int(y_max), int(x_min):int(x_max)]\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "        # 레이블을 불러옵니다.\n",
    "        mask_label = self.mask_labels[index]\n",
    "        gender_label = self.gender_labels[index]\n",
    "        age_label = self.age_labels[index]\n",
    "        multi_class_label = mask_label * 6 + gender_label * 3 + age_label\n",
    "        \n",
    "        # 이미지를 Augmentation 시킵니다.\n",
    "        image_transform = self.transform(image=np.array(image))['image']\n",
    "        return image_transform, multi_class_label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "id": "040bfd03-c2a2-4c10-8dcb-8275fda718d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "train_transform = A.Compose([\n",
    "    A.CLAHE(p=0.5),\n",
    "    A.Resize(height=224, width=224),\n",
    "    A.Normalize(mean=(0.56, 0.524, 0.501), std=(0.233, 0.243, 0.246)),\n",
    "    ToTensorV2(),\n",
    "])\n",
    "\n",
    "val_transform = A.Compose([\n",
    "    A.Resize(height=224, width=224),\n",
    "    A.Normalize(mean=(0.56, 0.524, 0.501), std=(0.233, 0.243, 0.246)),\n",
    "    ToTensorV2(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "id": "fffbfa39-a514-40ad-87fa-bdbf4d759630",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MaskBaseDataset(img_dir=train_img_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "id": "f30c7167-5500-49b0-beac-a04bf21bb687",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train dataset과 validation dataset을 7:3 비율로 나눕니다.\n",
    "n_val = int(len(dataset) * 0.3)\n",
    "n_train = len(dataset) - n_val\n",
    "train_dataset, val_dataset = data.random_split(dataset, [n_train, n_val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "id": "a602700f-f037-4c96-9fb7-b8f06000bd19",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.dataset.set_transform(train_transform)\n",
    "val_dataset.dataset.set_transform(val_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "id": "f04b633e-cd87-4685-be22-1fa94a819824",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training dataloader은 데이터를 섞어주어야 합니다. (shuffle=True)\n",
    "train_loader = data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=12,\n",
    "    num_workers=0,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "val_loader = data.DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=64,\n",
    "    num_workers=0,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b892fb3-3a6b-4c5e-aa04-8be7c7807102",
   "metadata": {},
   "outputs": [],
   "source": [
    "images, labels = next(iter(train_loader))\n",
    "print(f'images shape: {images.shape}')\n",
    "print(f'labels shape: {labels.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7048447-ae76-44ae-b08a-a32f2cf86929",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "# Augmentation으로 이미지를 Normalize했기 때문에, 역으로 다시 Normalize\n",
    "inv_normalize = transforms.Normalize(\n",
    "    mean=[-m / s for m, s in zip((0.56, 0.524, 0.501), (0.233, 0.243, 0.246))],\n",
    "    std=[1 / s for s in (0.233, 0.243, 0.246)]\n",
    ")\n",
    "\n",
    "n_rows, n_cols = 4, 3\n",
    "\n",
    "fig, axes = plt.subplots(n_rows, n_cols, sharex=True, sharey=True, figsize=(16, 24))\n",
    "for i in range(n_rows*n_cols):\n",
    "    axes[i%n_rows][i//(n_cols+1)].imshow(inv_normalize(images[i]).permute(1, 2, 0))\n",
    "    #axes[i%n_rows][i//(n_cols+1)].set_title(f'Label: {labels[i]}', color='r')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34517142-067f-4932-a87d-107b4a30dc95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31fbb00e-6903-421d-84f2-78d87d85eb2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 베이스라인 코드에 맞게 경로 수정\n",
    "bounding_box = pd.read_csv('./bounding_box.csv')\n",
    "image_path = bounding_box[\"img_paths\"].values\n",
    "for i in range(len(image_path)):\n",
    "    image_path[i] = '/opt/ml'+image_path[i][1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b4448a-5d69-40e2-ae63-13cd506c3e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "bounding_box.to_csv('./bounding_box.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e67d398-0c63-4493-b830-c473b426cbd5",
   "metadata": {},
   "source": [
    "# face bounding boxes for test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "id": "3ee24cec-d8c8-4f61-84bf-13d4ae73577e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = './input/data/eval'\n",
    "img_root = os.path.join(data_dir, 'images')\n",
    "info_path = os.path.join(data_dir, 'info.csv')\n",
    "info = pd.read_csv(info_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "id": "2655d1e6-4592-4d44-83da-cb59b05432e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestDataset(Dataset):\n",
    "    def __init__(self, img_paths):\n",
    "        self.img_paths = img_paths\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        image_path = self.img_paths[index]\n",
    "        image = np.array(Image.open(image_path))\n",
    "        return image, image_path\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "id": "507de5ee-7bdc-4350-9951-77b42a047fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_paths = [os.path.join(img_root, img_id) for img_id in info.ImageID]\n",
    "dataset = TestDataset(img_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "id": "e09abc2a-5aa8-4882-af0a-404f4dfc208d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/torch/utils/data/dataset.py:342: UserWarning: Length of split at index 1 is 0. This might result in an empty dataset.\n",
      "  warnings.warn(f\"Length of split at index {i} is 0. \"\n"
     ]
    }
   ],
   "source": [
    "total_dataset, _ = data.random_split(dataset, [1, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "id": "28a05b41-8f4c-409f-9319-884372c33229",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_loader = data.DataLoader(\n",
    "    total_dataset,\n",
    "    batch_size=64,\n",
    "    num_workers=0,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "eca12926-7ab2-48b3-a884-248fc052b640",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_paths = []\n",
    "x_mins = []\n",
    "y_mins = []\n",
    "x_maxs = []\n",
    "y_maxs = []\n",
    "\n",
    "for batch in total_loader:\n",
    "    inputs, img_path = batch\n",
    "    \n",
    "    for i, img in enumerate(inputs):\n",
    "        img_paths.append(img_path[i])\n",
    "        annotations = face_detector.predict_jsons(np.array(img))\n",
    "        \n",
    "        try:\n",
    "            x_min, y_min, x_max, y_max = annotations[0][\"bbox\"]\n",
    "            x_mins.append(x_min)\n",
    "            y_mins.append(y_min)\n",
    "            x_maxs.append(x_max)\n",
    "            y_maxs.append(y_max)\n",
    "        except:\n",
    "            x_mins.append(0)\n",
    "            y_mins.append(0)\n",
    "            x_maxs.append(0)\n",
    "            y_maxs.append(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5afd9c1-cc15-431f-8456-d2682a45fad1",
   "metadata": {},
   "source": [
    "## test visulization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "3cb9a3d5-427c-4cd2-ab71-a8d8f0f2579e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For test\n",
    "x_min, y_min, x_max, y_max = x_mins[0], y_mins[0], x_maxs[0], y_maxs[0]\n",
    "test=np.array(Image.open(img_paths[0]))[int(y_min):int(y_max), int(x_min):int(x_max)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd60cbcf-68a5-47aa-a04b-294e2408f867",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (5,5))\n",
    "plt.imshow(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f1d107-70bc-4ffc-b4e0-76047e7fbfbc",
   "metadata": {},
   "source": [
    "## save as csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "7e541804-c4a4-45fb-94a8-78a5cfd3330e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.DataFrame({'img_paths':img_paths, 'x_min':x_mins, 'y_min':y_mins, 'x_max':x_maxs, 'y_max':y_maxs})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "id": "7ea39f4c-5275-4a98-916b-5dd387af0aee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>img_paths</th>\n",
       "      <th>x_min</th>\n",
       "      <th>y_min</th>\n",
       "      <th>x_max</th>\n",
       "      <th>y_max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>./input/data/eval/images/47f2a06e62e3d9ba5cfd2...</td>\n",
       "      <td>113.73</td>\n",
       "      <td>74.34</td>\n",
       "      <td>275.24</td>\n",
       "      <td>291.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>./input/data/eval/images/0d2acf1953d2aa6a72142...</td>\n",
       "      <td>119.71</td>\n",
       "      <td>154.21</td>\n",
       "      <td>257.00</td>\n",
       "      <td>347.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>./input/data/eval/images/d055af2eabf67333dd4cc...</td>\n",
       "      <td>143.29</td>\n",
       "      <td>81.19</td>\n",
       "      <td>305.78</td>\n",
       "      <td>302.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>./input/data/eval/images/969776304bfaea84c23e2...</td>\n",
       "      <td>130.99</td>\n",
       "      <td>194.98</td>\n",
       "      <td>245.81</td>\n",
       "      <td>357.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>./input/data/eval/images/afa73b5d5c38ff2bdbcdf...</td>\n",
       "      <td>91.20</td>\n",
       "      <td>129.08</td>\n",
       "      <td>290.58</td>\n",
       "      <td>406.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12595</th>\n",
       "      <td>./input/data/eval/images/73f0a9543f05a8cb00ed0...</td>\n",
       "      <td>126.44</td>\n",
       "      <td>198.54</td>\n",
       "      <td>267.49</td>\n",
       "      <td>399.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12596</th>\n",
       "      <td>./input/data/eval/images/3010891cb4c98e957cf49...</td>\n",
       "      <td>124.21</td>\n",
       "      <td>173.91</td>\n",
       "      <td>256.05</td>\n",
       "      <td>356.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12597</th>\n",
       "      <td>./input/data/eval/images/6f5a8fa9a90040843f2ba...</td>\n",
       "      <td>115.99</td>\n",
       "      <td>189.46</td>\n",
       "      <td>251.58</td>\n",
       "      <td>379.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12598</th>\n",
       "      <td>./input/data/eval/images/c09c47d5799ee8c8e4150...</td>\n",
       "      <td>124.48</td>\n",
       "      <td>167.24</td>\n",
       "      <td>261.25</td>\n",
       "      <td>352.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12599</th>\n",
       "      <td>./input/data/eval/images/d1ab5481789ad54d24ae1...</td>\n",
       "      <td>129.37</td>\n",
       "      <td>162.02</td>\n",
       "      <td>251.26</td>\n",
       "      <td>334.44</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12600 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               img_paths   x_min   y_min  \\\n",
       "0      ./input/data/eval/images/47f2a06e62e3d9ba5cfd2...  113.73   74.34   \n",
       "1      ./input/data/eval/images/0d2acf1953d2aa6a72142...  119.71  154.21   \n",
       "2      ./input/data/eval/images/d055af2eabf67333dd4cc...  143.29   81.19   \n",
       "3      ./input/data/eval/images/969776304bfaea84c23e2...  130.99  194.98   \n",
       "4      ./input/data/eval/images/afa73b5d5c38ff2bdbcdf...   91.20  129.08   \n",
       "...                                                  ...     ...     ...   \n",
       "12595  ./input/data/eval/images/73f0a9543f05a8cb00ed0...  126.44  198.54   \n",
       "12596  ./input/data/eval/images/3010891cb4c98e957cf49...  124.21  173.91   \n",
       "12597  ./input/data/eval/images/6f5a8fa9a90040843f2ba...  115.99  189.46   \n",
       "12598  ./input/data/eval/images/c09c47d5799ee8c8e4150...  124.48  167.24   \n",
       "12599  ./input/data/eval/images/d1ab5481789ad54d24ae1...  129.37  162.02   \n",
       "\n",
       "        x_max   y_max  \n",
       "0      275.24  291.96  \n",
       "1      257.00  347.04  \n",
       "2      305.78  302.25  \n",
       "3      245.81  357.61  \n",
       "4      290.58  406.33  \n",
       "...       ...     ...  \n",
       "12595  267.49  399.15  \n",
       "12596  256.05  356.67  \n",
       "12597  251.58  379.02  \n",
       "12598  261.25  352.04  \n",
       "12599  251.26  334.44  \n",
       "\n",
       "[12600 rows x 5 columns]"
      ]
     },
     "execution_count": 337,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "id": "099401e5-a5ca-4c9d-97e0-edf881c7e067",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 경로 수정\n",
    "image_path = df_test[\"img_paths\"].values\n",
    "for i in range(len(image_path)):\n",
    "    image_path[i] = '/opt/ml'+image_path[i][1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "id": "f2fa7440-1ae5-45f2-a73a-0aa4837a4549",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.to_csv('./bounding_box_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70249058-0fd3-4d47-a5bb-8b98682d8443",
   "metadata": {},
   "source": [
    "## test visulization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "id": "c9ecdad0-9198-4342-ac9c-cfde17663a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "bounding_box_test = pd.read_csv('./bounding_box_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "id": "0e96a5cc-46d8-44c4-b112-4b870a5e2d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "bounding_box_test.set_index('img_paths', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "id": "afd1d26f-9912-47ce-a21d-1820023cb510",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88.99 172.31 262.33 420.35\n"
     ]
    }
   ],
   "source": [
    "x_min, y_min, x_max, y_max = bounding_box_test.loc[image_path[16]].values\n",
    "print(x_min, y_min, x_max, y_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba2bcba7-1b82-4b0d-bb1e-e14933619d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test=np.array(Image.open(img_paths[16]))[int(y_min):int(y_max), int(x_min):int(x_max)]\n",
    "\n",
    "plt.figure(figsize = (5,5))\n",
    "plt.imshow(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a87783-849e-4b4b-b0a7-c8cd46d67bc2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "920116b6fb34698f10a83cf3d191bb526e2b83df2edc9c5769a3bda3d85fd392"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
