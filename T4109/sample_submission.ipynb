{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "about-heavy",
   "metadata": {},
   "source": [
    "## 0. Library 불러오기 및 경로설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cubic-scoop",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import Resize, ToTensor, Normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "278ca345",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import multiprocessing\n",
    "import os\n",
    "from importlib import import_module\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from dataset import TestDataset, MaskBaseDataset\n",
    "\n",
    "from torchinfo import summary\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "68bf39ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트 데이터셋 폴더 경로를 지정해주세요.\n",
    "test_dir = '/opt/ml/input/data/eval'\n",
    "\n",
    "data_dir = '/opt/ml/input/data/eval'\n",
    "model_dir = './model/SEnet01'\n",
    "output_dir = './output'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d88f91e6",
   "metadata": {},
   "source": [
    "### <inference.py start>\n",
    "<!--  -->\n",
    "<!-- <font size=\"5\">inference.py</font> -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "84910cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(saved_model, num_classes, device):\n",
    "    # model_cls = model.MySenet50\n",
    "    model_cls = getattr(import_module(\"model\"), 'MySenet50')\n",
    "    model = model_cls(\n",
    "        num_classes=num_classes\n",
    "    )\n",
    "\n",
    "    # tarpath = os.path.join(saved_model, 'best.tar.gz')\n",
    "    # tar = tarfile.open(tarpath, 'r:gz')\n",
    "    # tar.extractall(path=saved_model)\n",
    "\n",
    "    best_model_path = os.path.join(saved_model, 'best.pth')\n",
    "    # print(best_model_path) # ./model/SEnet01/best.pth\n",
    "    # print()\n",
    "    model.load_state_dict(torch.load(best_model_path, map_location=device))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "174d1e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "num_classes = MaskBaseDataset.num_classes  # 18\n",
    "model = load_model(model_dir, num_classes, device).to(device)\n",
    "model.eval()\n",
    "\n",
    "img_root = os.path.join(data_dir, 'images')   # '/opt/ml/input/data/eval/images'\n",
    "info_path = os.path.join(data_dir, 'info.csv')\n",
    "info = pd.read_csv(info_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "06c3e887",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ImageID</th>\n",
       "      <th>ans</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cbc5c6e168e63498590db46022617123f1fe1268.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0e72482bf56b3581c081f7da2a6180b8792c7089.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>b549040c49190cedc41327748aeb197c1670f14d.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4f9cb2a045c6d5b9e50ad3459ea7b791eb6e18bc.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>248428d9a4a5b6229a7081c32851b90cb8d38d0c.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12595</th>\n",
       "      <td>d71d4570505d6af8f777690e63edfa8d85ea4476.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12596</th>\n",
       "      <td>6cf1300e8e218716728d5820c0bab553306c2cfd.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12597</th>\n",
       "      <td>8140edbba31c3a824e817e6d5fb95343199e2387.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12598</th>\n",
       "      <td>030d439efe6fb5a7bafda45a393fc19f2bf57f54.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12599</th>\n",
       "      <td>f1e0b9594ae9f72571f0a9dc67406ad41f2edab0.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12600 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            ImageID  ans\n",
       "0      cbc5c6e168e63498590db46022617123f1fe1268.jpg    0\n",
       "1      0e72482bf56b3581c081f7da2a6180b8792c7089.jpg    0\n",
       "2      b549040c49190cedc41327748aeb197c1670f14d.jpg    0\n",
       "3      4f9cb2a045c6d5b9e50ad3459ea7b791eb6e18bc.jpg    0\n",
       "4      248428d9a4a5b6229a7081c32851b90cb8d38d0c.jpg    0\n",
       "...                                             ...  ...\n",
       "12595  d71d4570505d6af8f777690e63edfa8d85ea4476.jpg    0\n",
       "12596  6cf1300e8e218716728d5820c0bab553306c2cfd.jpg    0\n",
       "12597  8140edbba31c3a824e817e6d5fb95343199e2387.jpg    0\n",
       "12598  030d439efe6fb5a7bafda45a393fc19f2bf57f54.jpg    0\n",
       "12599  f1e0b9594ae9f72571f0a9dc67406ad41f2edab0.jpg    0\n",
       "\n",
       "[12600 rows x 2 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e1725a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_paths = [os.path.join(img_root, img_id) for img_id in info.ImageID]\n",
    "dataset = TestDataset(img_paths,(128,96))\n",
    "loader = torch.utils.data.DataLoader(\n",
    "    dataset=dataset,\n",
    "    batch_size=64,\n",
    "    num_workers=multiprocessing.cpu_count() // 2,\n",
    "    shuffle=False,\n",
    "    pin_memory=use_cuda,\n",
    "    drop_last=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fa6e60cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64\n",
      "torch.Size([3, 128, 96])\n",
      "<class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "a = next(iter(loader))\n",
    "print(len(a))\n",
    "print(a[0].shape)\n",
    "print(type(a))\n",
    "\n",
    "my_img = a[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c39b723d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 128, 96])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "156edefd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 18])\n",
      "tensor([-14.7595,  -8.2747,  -8.8844, -15.5968, -17.6608, -14.9487, -11.9341,\n",
      "         -5.9835,  -8.6265, -13.5038, -13.1029, -15.9331,  -2.2673,   3.2675,\n",
      "          0.0394,  -4.0347,  -4.0085,  -3.3657], device='cuda:0')\n",
      "tensor(13, device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = []\n",
    "with torch.no_grad():\n",
    "    for index, images in enumerate(loader):\n",
    "        images = images.to(device)\n",
    "        pred = model.forward(images)\n",
    "        print(pred.shape)\n",
    "        print(pred[0])\n",
    "        pred = pred.argmax(dim=-1)\n",
    "        print(pred[0])\n",
    "        preds.extend(pred.cpu().numpy())\n",
    "        if index == 0:\n",
    "            break\n",
    "len(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fb01b094",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./output/output.csv\n"
     ]
    }
   ],
   "source": [
    "test = info[:5]\n",
    "# print(test)\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "save_path = os.path.join(output_dir, f'output.csv')\n",
    "print(save_path)\n",
    "info.to_csv(save_path, index = False)\n",
    "\n",
    "# output_dir = './output'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a1efb241",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Length of values (64) does not match length of index (12600)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m info[\u001b[39m'\u001b[39;49m\u001b[39mans\u001b[39;49m\u001b[39m'\u001b[39;49m] \u001b[39m=\u001b[39m preds\n\u001b[1;32m      2\u001b[0m save_path \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(output_dir, \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39moutput.csv\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m info\u001b[39m.\u001b[39mto_csv(save_path, )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/pandas/core/frame.py:3163\u001b[0m, in \u001b[0;36mDataFrame.__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   3160\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_setitem_array(key, value)\n\u001b[1;32m   3161\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   3162\u001b[0m     \u001b[39m# set column\u001b[39;00m\n\u001b[0;32m-> 3163\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_set_item(key, value)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/pandas/core/frame.py:3242\u001b[0m, in \u001b[0;36mDataFrame._set_item\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   3232\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   3233\u001b[0m \u001b[39mAdd series to DataFrame in specified column.\u001b[39;00m\n\u001b[1;32m   3234\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3239\u001b[0m \u001b[39mensure homogeneity.\u001b[39;00m\n\u001b[1;32m   3240\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   3241\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ensure_valid_index(value)\n\u001b[0;32m-> 3242\u001b[0m value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sanitize_column(key, value)\n\u001b[1;32m   3243\u001b[0m NDFrame\u001b[39m.\u001b[39m_set_item(\u001b[39mself\u001b[39m, key, value)\n\u001b[1;32m   3245\u001b[0m \u001b[39m# check if we are modifying a copy\u001b[39;00m\n\u001b[1;32m   3246\u001b[0m \u001b[39m# try to set first as we want an invalid\u001b[39;00m\n\u001b[1;32m   3247\u001b[0m \u001b[39m# value exception to occur first\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/pandas/core/frame.py:3899\u001b[0m, in \u001b[0;36mDataFrame._sanitize_column\u001b[0;34m(self, key, value, broadcast)\u001b[0m\n\u001b[1;32m   3894\u001b[0m     value \u001b[39m=\u001b[39m sanitize_index(value, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindex)\n\u001b[1;32m   3896\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(value, Index) \u001b[39mor\u001b[39;00m is_sequence(value):\n\u001b[1;32m   3897\u001b[0m \n\u001b[1;32m   3898\u001b[0m     \u001b[39m# turn me into an ndarray\u001b[39;00m\n\u001b[0;32m-> 3899\u001b[0m     value \u001b[39m=\u001b[39m sanitize_index(value, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mindex)\n\u001b[1;32m   3900\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(value, (np\u001b[39m.\u001b[39mndarray, Index)):\n\u001b[1;32m   3901\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(value, \u001b[39mlist\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(value) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/pandas/core/internals/construction.py:751\u001b[0m, in \u001b[0;36msanitize_index\u001b[0;34m(data, index)\u001b[0m\n\u001b[1;32m    746\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    747\u001b[0m \u001b[39mSanitize an index type to return an ndarray of the underlying, pass\u001b[39;00m\n\u001b[1;32m    748\u001b[0m \u001b[39mthrough a non-Index.\u001b[39;00m\n\u001b[1;32m    749\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    750\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(data) \u001b[39m!=\u001b[39m \u001b[39mlen\u001b[39m(index):\n\u001b[0;32m--> 751\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    752\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mLength of values \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    753\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m(\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(data)\u001b[39m}\u001b[39;00m\u001b[39m) \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    754\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mdoes not match length of index \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    755\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m(\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(index)\u001b[39m}\u001b[39;00m\u001b[39m)\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    756\u001b[0m     )\n\u001b[1;32m    758\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, np\u001b[39m.\u001b[39mndarray):\n\u001b[1;32m    759\u001b[0m \n\u001b[1;32m    760\u001b[0m     \u001b[39m# coerce datetimelike types\u001b[39;00m\n\u001b[1;32m    761\u001b[0m     \u001b[39mif\u001b[39;00m data\u001b[39m.\u001b[39mdtype\u001b[39m.\u001b[39mkind \u001b[39min\u001b[39;00m [\u001b[39m\"\u001b[39m\u001b[39mM\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mm\u001b[39m\u001b[39m\"\u001b[39m]:\n",
      "\u001b[0;31mValueError\u001b[0m: Length of values (64) does not match length of index (12600)"
     ]
    }
   ],
   "source": [
    "info['ans'] = preds\n",
    "save_path = os.path.join(output_dir, f'output.csv')\n",
    "info.to_csv(save_path, )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f55e7e",
   "metadata": {},
   "source": [
    "### <inference.py end>\n",
    "<!--  -->\n",
    "<!-- <font size=\"5\">inference.py</font> -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quiet-organizer",
   "metadata": {},
   "source": [
    "## 1. Model 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acknowledged-easter",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(nn.Module):\n",
    "    def __init__(self, num_classes: int = 1000):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(32, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.features(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "domestic-channels",
   "metadata": {},
   "source": [
    "## 2. Test Dataset 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extensive-north",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestDataset(Dataset):\n",
    "    def __init__(self, img_paths, transform):\n",
    "        self.img_paths = img_paths\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image = Image.open(self.img_paths[index])\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continued-feelings",
   "metadata": {},
   "source": [
    "## 3. Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coral-shade",
   "metadata": {},
   "outputs": [],
   "source": [
    "# meta 데이터와 이미지 경로를 불러옵니다.\n",
    "submission = pd.read_csv(os.path.join(test_dir, 'info.csv'))\n",
    "image_dir = os.path.join(test_dir, 'images')\n",
    "\n",
    "# Test Dataset 클래스 객체를 생성하고 DataLoader를 만듭니다.\n",
    "image_paths = [os.path.join(image_dir, img_id) for img_id in submission.ImageID]\n",
    "transform = transforms.Compose([\n",
    "    Resize((512, 384), Image.BILINEAR),\n",
    "    ToTensor(),\n",
    "    Normalize(mean=(0.5, 0.5, 0.5), std=(0.2, 0.2, 0.2)),\n",
    "])\n",
    "dataset = TestDataset(image_paths, transform)\n",
    "\n",
    "loader = DataLoader(\n",
    "    dataset,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# 모델을 정의합니다. (학습한 모델이 있다면 torch.load로 모델을 불러주세요!)\n",
    "device = torch.device('cuda')\n",
    "model = MyModel(num_classes=18).to(device)\n",
    "model.eval()\n",
    "\n",
    "# 모델이 테스트 데이터셋을 예측하고 결과를 저장합니다.\n",
    "all_predictions = []\n",
    "for images in loader:\n",
    "    with torch.no_grad():\n",
    "        images = images.to(device)\n",
    "        pred = model(images)\n",
    "        pred = pred.argmax(dim=-1)\n",
    "        all_predictions.extend(pred.cpu().numpy())\n",
    "submission['ans'] = all_predictions\n",
    "\n",
    "# 제출할 파일을 저장합니다.\n",
    "submission.to_csv(os.path.join(test_dir, 'submission.csv'), index=False)\n",
    "print('test inference is done!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
