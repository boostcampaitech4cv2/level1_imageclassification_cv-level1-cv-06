{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import & config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## utf-8 encoding?\n",
    "## 한글 업로드 테스트\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from glob import glob\n",
    "import requests\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from tqdm.notebook import tqdm\n",
    "from time import time\n",
    "from torch.optim import *\n",
    "import timm\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations import *\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Configurations\n",
    "\n",
    "train_folder_dir = '/opt/ml/repo/level1_imageclassification_cv-level1-cv-06/T4064/dataset/train'\n",
    "eval_folder_dir = '/opt/ml/repo/level1_imageclassification_cv-level1-cv-06/T4064/dataset/eval'\n",
    "\n",
    "train_imgs_dir = f'{train_folder_dir}/images'\n",
    "train_labels_path = f'{train_folder_dir}/train.csv'\n",
    "eval_imgs_dir = f'{eval_folder_dir}/images'\n",
    "eval_labels_path = f'{eval_folder_dir}/info.csv'\n",
    "\n",
    "cfg = {\n",
    "    'model_name' : 'resnet34',\n",
    "    'epochs' : 25,\n",
    "    'lr' : 1e-4,\n",
    "    'seed':42,\n",
    "    'num_classes':18\n",
    "    }\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>gender</th>\n",
       "      <th>race</th>\n",
       "      <th>age</th>\n",
       "      <th>path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000001</td>\n",
       "      <td>female</td>\n",
       "      <td>Asian</td>\n",
       "      <td>45</td>\n",
       "      <td>000001_female_Asian_45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000002</td>\n",
       "      <td>female</td>\n",
       "      <td>Asian</td>\n",
       "      <td>52</td>\n",
       "      <td>000002_female_Asian_52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000004</td>\n",
       "      <td>male</td>\n",
       "      <td>Asian</td>\n",
       "      <td>54</td>\n",
       "      <td>000004_male_Asian_54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>000005</td>\n",
       "      <td>female</td>\n",
       "      <td>Asian</td>\n",
       "      <td>58</td>\n",
       "      <td>000005_female_Asian_58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>000006</td>\n",
       "      <td>female</td>\n",
       "      <td>Asian</td>\n",
       "      <td>59</td>\n",
       "      <td>000006_female_Asian_59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2695</th>\n",
       "      <td>006954</td>\n",
       "      <td>male</td>\n",
       "      <td>Asian</td>\n",
       "      <td>19</td>\n",
       "      <td>006954_male_Asian_19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2696</th>\n",
       "      <td>006955</td>\n",
       "      <td>male</td>\n",
       "      <td>Asian</td>\n",
       "      <td>19</td>\n",
       "      <td>006955_male_Asian_19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2697</th>\n",
       "      <td>006956</td>\n",
       "      <td>male</td>\n",
       "      <td>Asian</td>\n",
       "      <td>19</td>\n",
       "      <td>006956_male_Asian_19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2698</th>\n",
       "      <td>006957</td>\n",
       "      <td>male</td>\n",
       "      <td>Asian</td>\n",
       "      <td>20</td>\n",
       "      <td>006957_male_Asian_20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2699</th>\n",
       "      <td>006959</td>\n",
       "      <td>male</td>\n",
       "      <td>Asian</td>\n",
       "      <td>19</td>\n",
       "      <td>006959_male_Asian_19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2700 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  gender   race  age                    path\n",
       "0     000001  female  Asian   45  000001_female_Asian_45\n",
       "1     000002  female  Asian   52  000002_female_Asian_52\n",
       "2     000004    male  Asian   54    000004_male_Asian_54\n",
       "3     000005  female  Asian   58  000005_female_Asian_58\n",
       "4     000006  female  Asian   59  000006_female_Asian_59\n",
       "...      ...     ...    ...  ...                     ...\n",
       "2695  006954    male  Asian   19    006954_male_Asian_19\n",
       "2696  006955    male  Asian   19    006955_male_Asian_19\n",
       "2697  006956    male  Asian   19    006956_male_Asian_19\n",
       "2698  006957    male  Asian   20    006957_male_Asian_20\n",
       "2699  006959    male  Asian   19    006959_male_Asian_19\n",
       "\n",
       "[2700 rows x 5 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(train_labels_path)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## csv 파일을 기반으로 img_path에 해당하는 class를 label_codes, img_paths 리스트에 저장하는 코드블럭\n",
    "\n",
    "df = pd.read_csv(train_labels_path)\n",
    "\n",
    "def get_label(mask_tag, gender, age):\n",
    "    gender = gender.lower()\n",
    "    label = 0\n",
    "    if mask_tag == 'Wear' and gender == 'male' and age<30: # 0:\n",
    "        label = 0\n",
    "    elif mask_tag == 'Wear' and gender == 'male' and 30<=age<60: # 1\n",
    "        label = 1\n",
    "    elif mask_tag == 'Wear' and gender == 'male' and age>=60: # 2\n",
    "        label = 2\n",
    "    elif mask_tag == 'Wear' and gender == 'female' and age<30: # 3\n",
    "        label = 3\n",
    "    elif mask_tag == 'Wear' and gender == 'female' and 30<=age<60: # 4\n",
    "        label = 4\n",
    "    elif mask_tag == 'Wear' and gender == 'female' and age>=60: # 5\n",
    "        label = 5\n",
    "    elif mask_tag == 'Incorrect' and gender == 'male' and age<30: # 6\n",
    "        label = 6\n",
    "    elif mask_tag == 'Incorrect' and gender == 'male' and 30<=age<60: # 7\n",
    "        label = 7\n",
    "    elif mask_tag == 'Incorrect' and gender == 'male' and age>=60: # 8\n",
    "        label = 8\n",
    "    elif mask_tag == 'Incorrect' and gender == 'female' and age<30: # 9\n",
    "        label = 9\n",
    "    elif mask_tag == 'Incorrect' and gender == 'female' and 30<=age<60: # 10\n",
    "        label = 10\n",
    "    elif mask_tag == 'Incorrect' and gender == 'female' and age>=60: # 11\n",
    "        label = 11\n",
    "    elif mask_tag == 'Not Wear' and gender == 'male' and age<30: # 12\n",
    "        label = 12\n",
    "    elif mask_tag == 'Not Wear' and gender == 'male' and 30<=age<60: # 13\n",
    "        label = 13\n",
    "    elif mask_tag == 'Not Wear' and gender == 'male' and age>=60: # 14\n",
    "        label = 14\n",
    "    elif mask_tag == 'Not Wear' and gender == 'female' and age<30: # 15\n",
    "        label = 15\n",
    "    elif mask_tag == 'Not Wear' and gender == 'female' and 30<=age<60: # 16\n",
    "        label = 16\n",
    "    elif mask_tag == 'Not Wear' and gender == 'female' and age>=60: # 17\n",
    "        label = 17\n",
    "    else:\n",
    "        raise ValueError\n",
    "    return label\n",
    "\n",
    "label_codes = []\n",
    "img_paths = []\n",
    "\n",
    "for gender, age, folder_name in zip(df['gender'], df['age'], df['path']):\n",
    "    folder_path = os.path.join(train_imgs_dir, folder_name)\n",
    "    files = os.listdir(folder_path)\n",
    "    for file_name in files:\n",
    "        img_path = os.path.join(folder_path,file_name)\n",
    "        if 'incorrect' in file_name:\n",
    "            label = get_label('Incorrect', gender, age)\n",
    "        elif 'mask' in file_name:\n",
    "            label = get_label('Wear', gender, age)\n",
    "        elif 'normal' in file_name:\n",
    "            label = get_label('Not Wear', gender, age)\n",
    "        else:\n",
    "            raise ValueError\n",
    "        img_paths.append(img_path)\n",
    "        label_codes.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/ml/repo/level1_imageclassification_cv-level1-cv-06/T4064/dataset/train/images/000001_female_Asian_45/mask1.jpg 4\n"
     ]
    }
   ],
   "source": [
    "for path, code in zip(img_paths, label_codes):\n",
    "    print(path, code)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     2745\n",
       "1     2050\n",
       "2      415\n",
       "3     3660\n",
       "4     4085\n",
       "5      545\n",
       "6      549\n",
       "7      410\n",
       "8       83\n",
       "9      732\n",
       "10     817\n",
       "11     109\n",
       "12     549\n",
       "13     410\n",
       "14      83\n",
       "15     732\n",
       "16     817\n",
       "17     109\n",
       "dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 나이 클래스별 분포를 확인\n",
    "counts = pd.DataFrame(label_codes).value_counts().sort_index()\n",
    "counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 데이터 스플릿\n",
    "train_paths, valid_paths, train_labels, valid_labels = train_test_split(img_paths, label_codes,\n",
    "                                                                        train_size = 0.7,\n",
    "                                                                        shuffle = True,\n",
    "                                                                        random_state = cfg['seed'],\n",
    "                                                                        stratify=label_codes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_csv = pd.DataFrame(list(zip(train_paths, train_labels)), columns=['img_path', 'class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_csv = pd.DataFrame(list(zip(valid_paths, valid_labels)), columns=['img_path', 'class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13230 5670\n"
     ]
    }
   ],
   "source": [
    "print(len(train_csv), len(valid_csv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>img_path</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/opt/ml/repo/level1_imageclassification_cv-lev...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/opt/ml/repo/level1_imageclassification_cv-lev...</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/opt/ml/repo/level1_imageclassification_cv-lev...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/opt/ml/repo/level1_imageclassification_cv-lev...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/opt/ml/repo/level1_imageclassification_cv-lev...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13225</th>\n",
       "      <td>/opt/ml/repo/level1_imageclassification_cv-lev...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13226</th>\n",
       "      <td>/opt/ml/repo/level1_imageclassification_cv-lev...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13227</th>\n",
       "      <td>/opt/ml/repo/level1_imageclassification_cv-lev...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13228</th>\n",
       "      <td>/opt/ml/repo/level1_imageclassification_cv-lev...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13229</th>\n",
       "      <td>/opt/ml/repo/level1_imageclassification_cv-lev...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13230 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                img_path  class\n",
       "0      /opt/ml/repo/level1_imageclassification_cv-lev...      0\n",
       "1      /opt/ml/repo/level1_imageclassification_cv-lev...     16\n",
       "2      /opt/ml/repo/level1_imageclassification_cv-lev...      4\n",
       "3      /opt/ml/repo/level1_imageclassification_cv-lev...      4\n",
       "4      /opt/ml/repo/level1_imageclassification_cv-lev...      4\n",
       "...                                                  ...    ...\n",
       "13225  /opt/ml/repo/level1_imageclassification_cv-lev...      4\n",
       "13226  /opt/ml/repo/level1_imageclassification_cv-lev...      4\n",
       "13227  /opt/ml/repo/level1_imageclassification_cv-lev...      7\n",
       "13228  /opt/ml/repo/level1_imageclassification_cv-lev...      3\n",
       "13229  /opt/ml/repo/level1_imageclassification_cv-lev...      0\n",
       "\n",
       "[13230 rows x 2 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "## csv파일을 받아서 리턴하는 Custom Dataset Class\n",
    "\n",
    "class MaskDataSet(Dataset):#train_labels_path\n",
    "    def __init__(self, path_df, transforms=None):\n",
    "        self.transforms = transforms\n",
    "        self.path_df = path_df\n",
    "        if transforms is None: # transform이 없으면 자동으로 3개의 transform이 적용됨.\n",
    "            print('If transforms param is None automatically Resize(224), Normalize, ToTensorV2 Apply')\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        img_path = self.path_df['img_path'][index]\n",
    "        img = np.array(Image.open(img_path))\n",
    "        img_class = self.path_df['class'][index]\n",
    "        if self.transforms is not None:\n",
    "            img = self.transforms(image=img)[\"image\"]\n",
    "        else:\n",
    "            temp_transforms = A.Compose([A.Resize(height=224, width=224), A.Normalize(), ToTensorV2()])\n",
    "            img = temp_transforms(image=img)[\"image\"]\n",
    "        return img, img_class\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.path_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transforms = A.Compose([A.Resize(height=224, width=224),\n",
    "                              A.HorizontalFlip(p=0.5),\n",
    "                              A.RandomBrightnessContrast(p=0.5),\n",
    "                              A.GaussianBlur(p=0.5),\n",
    "                              A.GridDistortion(p=0.5),\n",
    "                              A.Rotate(limit=30, p=0.5),\n",
    "                              A.Normalize(mean=(0.56019358,0.52410121,0.501457),\n",
    "                                          std=(0.23318603,0.24300033,0.24567522)),\n",
    "                              ToTensorV2()])\n",
    "\n",
    "valid_transforms = A.Compose([A.Resize(height=224, width=224),\n",
    "                              A.Normalize(mean=(0.56019358,0.52410121,0.501457),\n",
    "                                          std=(0.23318603,0.24300033,0.24567522)),\n",
    "                              ToTensorV2()])\n",
    "\n",
    "train_dataset = MaskDataSet(train_csv, train_transforms)\n",
    "valid_dataset = MaskDataSet(valid_csv, valid_transforms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (act1): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (drop_block): Identity()\n",
       "      (act1): ReLU(inplace=True)\n",
       "      (aa): Identity()\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act2): ReLU(inplace=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (drop_block): Identity()\n",
       "      (act1): ReLU(inplace=True)\n",
       "      (aa): Identity()\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act2): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (drop_block): Identity()\n",
       "      (act1): ReLU(inplace=True)\n",
       "      (aa): Identity()\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act2): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (drop_block): Identity()\n",
       "      (act1): ReLU(inplace=True)\n",
       "      (aa): Identity()\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act2): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (drop_block): Identity()\n",
       "      (act1): ReLU(inplace=True)\n",
       "      (aa): Identity()\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act2): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (drop_block): Identity()\n",
       "      (act1): ReLU(inplace=True)\n",
       "      (aa): Identity()\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act2): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (drop_block): Identity()\n",
       "      (act1): ReLU(inplace=True)\n",
       "      (aa): Identity()\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act2): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (drop_block): Identity()\n",
       "      (act1): ReLU(inplace=True)\n",
       "      (aa): Identity()\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act2): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (drop_block): Identity()\n",
       "      (act1): ReLU(inplace=True)\n",
       "      (aa): Identity()\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act2): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (drop_block): Identity()\n",
       "      (act1): ReLU(inplace=True)\n",
       "      (aa): Identity()\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act2): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (drop_block): Identity()\n",
       "      (act1): ReLU(inplace=True)\n",
       "      (aa): Identity()\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act2): ReLU(inplace=True)\n",
       "    )\n",
       "    (4): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (drop_block): Identity()\n",
       "      (act1): ReLU(inplace=True)\n",
       "      (aa): Identity()\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act2): ReLU(inplace=True)\n",
       "    )\n",
       "    (5): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (drop_block): Identity()\n",
       "      (act1): ReLU(inplace=True)\n",
       "      (aa): Identity()\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act2): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (drop_block): Identity()\n",
       "      (act1): ReLU(inplace=True)\n",
       "      (aa): Identity()\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act2): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (drop_block): Identity()\n",
       "      (act1): ReLU(inplace=True)\n",
       "      (aa): Identity()\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act2): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (drop_block): Identity()\n",
       "      (act1): ReLU(inplace=True)\n",
       "      (aa): Identity()\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act2): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (global_pool): SelectAdaptivePool2d (pool_type=avg, flatten=Flatten(start_dim=1, end_dim=-1))\n",
       "  (fc): Linear(in_features=512, out_features=18, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_pretrain_model(config):\n",
    "    model_name = config['model_name']\n",
    "    model = timm.create_model(model_name=config['model_name'], pretrained=True, num_classes=config['num_classes'])\n",
    "    return model\n",
    "\n",
    "pretrained_model = get_pretrain_model(cfg)\n",
    "pretrained_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([13,  4,  7,  7, 14, 12,  6, 12,  8,  2, 12,  8,  6, 15, 13,  8, 13, 12,\n",
       "         4, 13,  1,  6,  1,  2,  6, 12,  7, 14, 13,  7,  6,  7,  8,  7, 17,  8,\n",
       "        14, 13, 14,  7, 13,  4,  6,  7, 13,  7,  7,  6,  7,  6,  9, 10,  7, 12,\n",
       "        10, 12, 14,  7, 13,  7, 13,  1,  7, 12], device='cuda:0')"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 테스트\n",
    "batch_images, batch_labels = next(iter(train_loader))\n",
    "torch.argmax(pretrained_model(batch_images.to(device)), dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optim = Adam(pretrained_model.parameters(), lr = cfg['lr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64\n"
     ]
    }
   ],
   "source": [
    "for iter_idx, (train_imgs, train_labels) in enumerate(train_loader, start=1):\n",
    "    print(len(train_labels))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_valid(cfg, model, train_loader, valid_loader):\n",
    "  result = {\"train_loss\" : [],\n",
    "            \"valid_loss\" : [],\n",
    "            \"valid_acc\":[],\n",
    "            \"valid_f1\":[]}\n",
    "  for epoch in tqdm((range(1,cfg['epochs']))):\n",
    "      model.train()\n",
    "      running_train_loss = []\n",
    "      running_valid_loss = []\n",
    "\n",
    "      preds = []\n",
    "      labels = []\n",
    "\n",
    "\n",
    "      for iter_idx, (train_imgs, train_labels) in enumerate(train_loader, start=1):\n",
    "          train_imgs, train_labels = train_imgs.to(device, dtype=torch.float), train_labels.to(device)\n",
    "          optim.zero_grad()\n",
    "          train_pred = model(train_imgs)\n",
    "          train_loss = criterion(train_pred, train_labels)\n",
    "          train_loss.backward()\n",
    "          optim.step()\n",
    "          running_train_loss.append(train_loss.cpu().item())\n",
    "  \n",
    "      with torch.no_grad():\n",
    "        for iter_idx, (valid_imgs, valid_labels) in enumerate(valid_loader, start=1):\n",
    "          model.eval()\n",
    "          valid_imgs, valid_labels = valid_imgs.to(device, dtype=torch.float), valid_labels.to(device)\n",
    "          \n",
    "          valid_logit = model(valid_imgs)\n",
    "          valid_loss = criterion(valid_logit, valid_labels)\n",
    "          valid_preds = valid_logit.argmax(dim=-1)\n",
    "          \n",
    "          running_valid_loss.append(valid_loss.cpu().item())\n",
    "          preds += valid_preds.detach().cpu().numpy().tolist()\n",
    "          labels += valid_labels.detach().cpu().numpy().tolist()\n",
    "\n",
    "      train_loss = np.mean(running_train_loss)\n",
    "      valid_loss = np.mean(running_valid_loss)\n",
    "      valid_f1 = f1_score(labels, preds, average='macro')\n",
    "      valid_acc = accuracy_score(labels,preds)\n",
    "      print(f\"{epoch}/{cfg['epochs']} : train_loss:{train_loss:.4f}, \\\n",
    "        valid_loss:{valid_loss:.4f}, valid_acc:{valid_acc:.2f}, valid_f1:{valid_f1:.2f}\")\n",
    "      result['train_loss'].append(train_loss)\n",
    "      result['valid_loss'].append(valid_loss)\n",
    "      result['valid_acc'].append(valid_acc)\n",
    "      result['valid_f1'].append(valid_f1)\n",
    "  return result, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e5a6187ebcd425ca6c39b7f635afc93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=24.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/25 : train_loss:0.6432,         valid_loss:0.2709, valid_acc:0.91, valid_f1:0.79\n",
      "2/25 : train_loss:0.2665,         valid_loss:0.2078, valid_acc:0.93, valid_f1:0.82\n",
      "3/25 : train_loss:0.1842,         valid_loss:0.1573, valid_acc:0.95, valid_f1:0.86\n",
      "4/25 : train_loss:0.1464,         valid_loss:0.0982, valid_acc:0.97, valid_f1:0.93\n",
      "5/25 : train_loss:0.1007,         valid_loss:0.0954, valid_acc:0.97, valid_f1:0.92\n",
      "6/25 : train_loss:0.0849,         valid_loss:0.0731, valid_acc:0.98, valid_f1:0.95\n",
      "7/25 : train_loss:0.0782,         valid_loss:0.0760, valid_acc:0.98, valid_f1:0.95\n",
      "8/25 : train_loss:0.0641,         valid_loss:0.0676, valid_acc:0.98, valid_f1:0.95\n",
      "9/25 : train_loss:0.0552,         valid_loss:0.0794, valid_acc:0.98, valid_f1:0.94\n",
      "10/25 : train_loss:0.0526,         valid_loss:0.0580, valid_acc:0.98, valid_f1:0.97\n",
      "11/25 : train_loss:0.0526,         valid_loss:0.0552, valid_acc:0.98, valid_f1:0.96\n",
      "12/25 : train_loss:0.0476,         valid_loss:0.0518, valid_acc:0.98, valid_f1:0.97\n",
      "13/25 : train_loss:0.0484,         valid_loss:0.0575, valid_acc:0.98, valid_f1:0.97\n",
      "14/25 : train_loss:0.0375,         valid_loss:0.0542, valid_acc:0.98, valid_f1:0.97\n",
      "15/25 : train_loss:0.0414,         valid_loss:0.0285, valid_acc:0.99, valid_f1:0.98\n",
      "16/25 : train_loss:0.0367,         valid_loss:0.0415, valid_acc:0.99, valid_f1:0.96\n",
      "17/25 : train_loss:0.0409,         valid_loss:0.0506, valid_acc:0.99, valid_f1:0.97\n",
      "18/25 : train_loss:0.0324,         valid_loss:0.0343, valid_acc:0.99, valid_f1:0.98\n",
      "19/25 : train_loss:0.0317,         valid_loss:0.0384, valid_acc:0.99, valid_f1:0.97\n",
      "20/25 : train_loss:0.0313,         valid_loss:0.0355, valid_acc:0.99, valid_f1:0.98\n",
      "21/25 : train_loss:0.0362,         valid_loss:0.0508, valid_acc:0.99, valid_f1:0.97\n",
      "22/25 : train_loss:0.0265,         valid_loss:0.0345, valid_acc:0.99, valid_f1:0.97\n",
      "23/25 : train_loss:0.0315,         valid_loss:0.0617, valid_acc:0.98, valid_f1:0.97\n",
      "24/25 : train_loss:0.0251,         valid_loss:0.0344, valid_acc:0.99, valid_f1:0.98\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result, fine_tuned_model = train_and_valid(cfg, pretrained_model, train_loader, valid_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (act1): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (drop_block): Identity()\n",
       "      (act1): ReLU(inplace=True)\n",
       "      (aa): Identity()\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act2): ReLU(inplace=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (drop_block): Identity()\n",
       "      (act1): ReLU(inplace=True)\n",
       "      (aa): Identity()\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act2): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (drop_block): Identity()\n",
       "      (act1): ReLU(inplace=True)\n",
       "      (aa): Identity()\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act2): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (drop_block): Identity()\n",
       "      (act1): ReLU(inplace=True)\n",
       "      (aa): Identity()\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act2): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (drop_block): Identity()\n",
       "      (act1): ReLU(inplace=True)\n",
       "      (aa): Identity()\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act2): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (drop_block): Identity()\n",
       "      (act1): ReLU(inplace=True)\n",
       "      (aa): Identity()\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act2): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (drop_block): Identity()\n",
       "      (act1): ReLU(inplace=True)\n",
       "      (aa): Identity()\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act2): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (drop_block): Identity()\n",
       "      (act1): ReLU(inplace=True)\n",
       "      (aa): Identity()\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act2): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (drop_block): Identity()\n",
       "      (act1): ReLU(inplace=True)\n",
       "      (aa): Identity()\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act2): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (drop_block): Identity()\n",
       "      (act1): ReLU(inplace=True)\n",
       "      (aa): Identity()\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act2): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (drop_block): Identity()\n",
       "      (act1): ReLU(inplace=True)\n",
       "      (aa): Identity()\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act2): ReLU(inplace=True)\n",
       "    )\n",
       "    (4): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (drop_block): Identity()\n",
       "      (act1): ReLU(inplace=True)\n",
       "      (aa): Identity()\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act2): ReLU(inplace=True)\n",
       "    )\n",
       "    (5): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (drop_block): Identity()\n",
       "      (act1): ReLU(inplace=True)\n",
       "      (aa): Identity()\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act2): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (drop_block): Identity()\n",
       "      (act1): ReLU(inplace=True)\n",
       "      (aa): Identity()\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act2): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (drop_block): Identity()\n",
       "      (act1): ReLU(inplace=True)\n",
       "      (aa): Identity()\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act2): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (drop_block): Identity()\n",
       "      (act1): ReLU(inplace=True)\n",
       "      (aa): Identity()\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act2): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (global_pool): SelectAdaptivePool2d (pool_type=avg, flatten=Flatten(start_dim=1, end_dim=-1))\n",
       "  (fc): Linear(in_features=512, out_features=18, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fine_tuned_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train_loss': [0.643201829586628,\n",
       "  0.2665444292164079,\n",
       "  0.18419367013778087,\n",
       "  0.14639450001399873,\n",
       "  0.10069840417607971,\n",
       "  0.08493318321444274,\n",
       "  0.0782051240026519,\n",
       "  0.06409295268608751,\n",
       "  0.05523639867883086,\n",
       "  0.052613029242965625,\n",
       "  0.052571648713831166,\n",
       "  0.0475611033183545,\n",
       "  0.04836737109766598,\n",
       "  0.037466579966094114,\n",
       "  0.041363985236095274,\n",
       "  0.03674749801653001,\n",
       "  0.04089784929483841,\n",
       "  0.03241687317509294,\n",
       "  0.03172994180242778,\n",
       "  0.03126147162880082,\n",
       "  0.03624531352084922,\n",
       "  0.02651915400950373,\n",
       "  0.031479752304230404,\n",
       "  0.025083742281062086],\n",
       " 'valid_loss': [0.2708845597304655,\n",
       "  0.2077813320113032,\n",
       "  0.15726927165569884,\n",
       "  0.09815930484092972,\n",
       "  0.09539370841524575,\n",
       "  0.07311616728103228,\n",
       "  0.07601986183481438,\n",
       "  0.06764652187611615,\n",
       "  0.07942292306132698,\n",
       "  0.05797798661881367,\n",
       "  0.05521092778124083,\n",
       "  0.051825904575166074,\n",
       "  0.05748810288443994,\n",
       "  0.054238120915389126,\n",
       "  0.028496551633488094,\n",
       "  0.04145536029952045,\n",
       "  0.05062448334346494,\n",
       "  0.03431963206143276,\n",
       "  0.03842626483927826,\n",
       "  0.035493052017801764,\n",
       "  0.05080728817030034,\n",
       "  0.034495448712272614,\n",
       "  0.06168925220387454,\n",
       "  0.034394027844458566],\n",
       " 'valid_acc': [0.9132275132275133,\n",
       "  0.9285714285714286,\n",
       "  0.9462081128747796,\n",
       "  0.9680776014109348,\n",
       "  0.9687830687830687,\n",
       "  0.9767195767195768,\n",
       "  0.9790123456790123,\n",
       "  0.9784832451499118,\n",
       "  0.9774250440917107,\n",
       "  0.9835978835978836,\n",
       "  0.982363315696649,\n",
       "  0.9843033509700176,\n",
       "  0.9830687830687831,\n",
       "  0.9828924162257495,\n",
       "  0.9924162257495591,\n",
       "  0.9890652557319224,\n",
       "  0.9873015873015873,\n",
       "  0.990652557319224,\n",
       "  0.9895943562610229,\n",
       "  0.9904761904761905,\n",
       "  0.9864197530864197,\n",
       "  0.9881834215167549,\n",
       "  0.9830687830687831,\n",
       "  0.991005291005291],\n",
       " 'valid_f1': [0.7901171857751033,\n",
       "  0.8187156172237807,\n",
       "  0.8589379170003327,\n",
       "  0.9281310824452316,\n",
       "  0.9225400368517465,\n",
       "  0.9477842358819104,\n",
       "  0.9529508339130371,\n",
       "  0.952583409218732,\n",
       "  0.9434677698860311,\n",
       "  0.9666774470268708,\n",
       "  0.9556812251674496,\n",
       "  0.9677819929516224,\n",
       "  0.9660085956597597,\n",
       "  0.968037085665986,\n",
       "  0.9823054795132546,\n",
       "  0.9649714927305928,\n",
       "  0.9687038392973688,\n",
       "  0.9768938062872695,\n",
       "  0.9709362685535852,\n",
       "  0.975639221914806,\n",
       "  0.9708314414325991,\n",
       "  0.9747153626465903,\n",
       "  0.9663380281652133,\n",
       "  0.9829461388802856]}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_file = {'model':fine_tuned_model.state_dict(),\n",
    "             'result':result,\n",
    "             'config':cfg}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = '/opt/ml/repo/level1_imageclassification_cv-level1-cv-06/T4064/checkpoints/1_resnet34.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(save_file, save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_imgs_dir = f'{eval_folder_dir}/images'\n",
    "os.listdir(eval_imgs_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_labels_path = f'{eval_folder_dir}/info.csv'\n",
    "eval_csv = pd.read_csv(eval_labels_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = []\n",
    "for file_name in eval_csv['ImageID']:\n",
    "    file_path = os.path.join(eval_imgs_dir, file_name)\n",
    "    img_path.append(file_path)\n",
    "    # print(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>img_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/opt/ml/repo/level1_imageclassification_cv-lev...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/opt/ml/repo/level1_imageclassification_cv-lev...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/opt/ml/repo/level1_imageclassification_cv-lev...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/opt/ml/repo/level1_imageclassification_cv-lev...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/opt/ml/repo/level1_imageclassification_cv-lev...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12595</th>\n",
       "      <td>/opt/ml/repo/level1_imageclassification_cv-lev...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12596</th>\n",
       "      <td>/opt/ml/repo/level1_imageclassification_cv-lev...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12597</th>\n",
       "      <td>/opt/ml/repo/level1_imageclassification_cv-lev...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12598</th>\n",
       "      <td>/opt/ml/repo/level1_imageclassification_cv-lev...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12599</th>\n",
       "      <td>/opt/ml/repo/level1_imageclassification_cv-lev...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12600 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                img_path\n",
       "0      /opt/ml/repo/level1_imageclassification_cv-lev...\n",
       "1      /opt/ml/repo/level1_imageclassification_cv-lev...\n",
       "2      /opt/ml/repo/level1_imageclassification_cv-lev...\n",
       "3      /opt/ml/repo/level1_imageclassification_cv-lev...\n",
       "4      /opt/ml/repo/level1_imageclassification_cv-lev...\n",
       "...                                                  ...\n",
       "12595  /opt/ml/repo/level1_imageclassification_cv-lev...\n",
       "12596  /opt/ml/repo/level1_imageclassification_cv-lev...\n",
       "12597  /opt/ml/repo/level1_imageclassification_cv-lev...\n",
       "12598  /opt/ml/repo/level1_imageclassification_cv-lev...\n",
       "12599  /opt/ml/repo/level1_imageclassification_cv-lev...\n",
       "\n",
       "[12600 rows x 1 columns]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_csv = pd.DataFrame(img_path)\n",
    "test_csv.columns = ['img_path']\n",
    "test_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestMaskDataSet(Dataset):#train_labels_path\n",
    "    def __init__(self, path_df, transforms=None):\n",
    "        self.transforms = transforms\n",
    "        self.path_df = path_df\n",
    "        if transforms is None: # transform이 없으면 자동으로 3개의 transform이 적용됨.\n",
    "            print('If transforms param is None automatically Resize(224), Normalize, ToTensorV2 Apply')\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        img_path = self.path_df['img_path'][index]\n",
    "        img = np.array(Image.open(img_path))\n",
    "        # img_class = self.path_df['class'][index]\n",
    "        if self.transforms is not None:\n",
    "            img = self.transforms(image=img)[\"image\"]\n",
    "        else:\n",
    "            temp_transforms = A.Compose([A.Resize(height=224, width=224), A.Normalize(), ToTensorV2()])\n",
    "            img = temp_transforms(image=img)[\"image\"]\n",
    "        return img # , img_class\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.path_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_transforms = A.Compose([A.Resize(height=224, width=224),\n",
    "                             A.Normalize(mean=(0.56019358,0.52410121,0.501457),\n",
    "                                         std=(0.23318603,0.24300033,0.24567522)),\n",
    "                             ToTensorV2()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = TestMaskDataSet(test_csv, transforms = test_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(model, test_loader):\n",
    "    total_test_preds = []\n",
    "    with torch.no_grad():\n",
    "        for iter_idx, test_imgs in enumerate(test_loader, start=1):\n",
    "            model.eval()\n",
    "            test_imgs = test_imgs.to(device, dtype=torch.float)\n",
    "            test_logit = model(test_imgs)\n",
    "            test_preds = test_logit.argmax(dim=-1)\n",
    "            total_test_preds += test_preds.detach().cpu().numpy().tolist()\n",
    "    return total_test_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_prediction_list = inference(fine_tuned_model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12600"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_prediction_list)\n",
    "# len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_submission = pd.read_csv('/opt/ml/repo/level1_imageclassification_cv-level1-cv-06/T4064/dataset/eval/submission.csv')\n",
    "df_submission['ans']=test_prediction_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12600"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(test_prediction_list == df_submission['ans']).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_file_path = '/opt/ml/repo/level1_imageclassification_cv-level1-cv-06/T4064/submission'\n",
    "df_submission.to_csv(submission_file_path + '/1_resnet34.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
