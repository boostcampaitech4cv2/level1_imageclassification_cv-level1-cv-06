{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "sound-volume",
   "metadata": {
    "id": "sound-volume"
   },
   "source": [
    "# Lesson 6 - Training & Inference\n",
    "**미션 개요**\n",
    "- 6강에서는 모델을 학습하고 추론하는 방법에 대해 알아보았습니다.\n",
    "- 이번 실습 자료에서는 다양한 Loss, Optimizer, Scheduler를 활용하는 방법을 알아봅니다.\n",
    "\n",
    "**미션의 목적 및 배경**\n",
    "- 파이토치 모델이 학습을 진행하기 위해서는 Loss, Optimizer, Scheduler 와 같은 다양한 장치들이 필요합니다.\n",
    "- Loss 를 통해 모델의 목표를 (Objective) 설정하고, 그 목표를 가기 위한 전략을 Optimizer 로 설정할 수 있습니다. 또한 Scheduler 를 통해 그 전략을 전체 Training Process 안에서 조정할 수 있습니다.\n",
    "- 각 기법들은 그 안에서도 각각의 목적에 맞게 다양하게 존재합니다.\n",
    "- 이런 기법들을 종합하여 전체 Training Process 가 탄생하게 됩니다.\n",
    "\n",
    "\n",
    "**미션 수행으로 얻어갈 수 있는 역량**\n",
    "- 전체 Training Process 를 만드는 방법에 대해 학습합니다.\n",
    "- Checkpoint, Early Stopping과 같은 학습을 도와주는 Callback 방법을 알아봅니다.\n",
    "- 그리고 Graident Accumulation 방법을 활용하여 학습을 진행해봅니다.\n",
    "\n",
    "**미션 핵심 내용**\n",
    "- Loss, Optimizer, Scheduler 를 포함한 다양한 학습 기법들을 사용하여 온전한 전체 Training Process 를 만들어봅니다.\n",
    "\n",
    "**데이터셋 개요 및 저작권 정보**\n",
    "- Face with Masks\n",
    "- 캠프 교육용 라이선스 : 교육 내에서만 활용해 주시고, 일부 이미지라도 외부에 노출되는 일은 없도록 부탁드립니다. (데이터셋에 대한 설명글이나, 데이터셋을 활용하여 학습한 weight 정도가 공개 가능합니다)\n",
    "\n",
    "**Reference**\n",
    "- [sumni blog post](https://sumniya.tistory.com/26)\n",
    "- [How to create a custom loss function in PyTorch](https://neptune.ai/blog/pytorch-loss-functions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gentle-appeal",
   "metadata": {
    "id": "gentle-appeal"
   },
   "source": [
    "## 1. Loss\n",
    "- Image Classification에 사용되는 다양한 loss 함수들이 존재합니다. 각 loss 함수는 목적이 있고 풀고자 하는 문제에 맞게 적용을 해야합니다.\n",
    "- Cross Entropy Loss는 두 분포간의 불확실성을 최소화 하는 목적을 가진 분류에 사용되는 일반적인 손실함수입니다.\n",
    "- Focal Loss는 Imbalanced Data 문제를 해결하기 위한 손실함수입니다. [참고](https://arxiv.org/pdf/1708.02002.pdf)\n",
    "- Label Smoothing은 학습 데이터의 representation을 더 잘나타내는데 도움을 줍니다. [참고](https://arxiv.org/pdf/1906.02629.pdf)\n",
    "- F1 Loss는 F1 score 향상을 목적으로 하는 손실함수입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90bf3a7f",
   "metadata": {
    "id": "90bf3a7f"
   },
   "source": [
    "###### 베이스라인 코드 train.py 파일 중 Loss 관련 부분.\n",
    "- default 로 cross_entropy로 설정되어 있지만 FocalLoss, LabelSmoothingLoss, F1Loss로 바꿔서 적용해 봅시다!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fca816b",
   "metadata": {
    "id": "3fca816b"
   },
   "outputs": [],
   "source": [
    "# -- loss & metric\n",
    "criterion = create_criterion(args.criterion)  # default: cross_entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8579fef",
   "metadata": {
    "id": "f8579fef"
   },
   "source": [
    "###### 베이스라인 코드 loss.py 파일.\n",
    "- FocalLoss, LabelSmoothingLoss, F1Loss, cross_entropy 중에 원하는 Loss 를 설정하여 학습시킬 수 있습니다. \n",
    "또한 직접 커스터마이징한 Loss를 추가해서 적용해 볼수 있습니다! (아래쪽 Reference를 참고하세요)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "closed-launch",
   "metadata": {
    "id": "closed-launch",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# https://discuss.pytorch.org/t/is-this-a-correct-implementation-for-focal-loss-in-pytorch/43327/8\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, weight=None,\n",
    "                 gamma=2., reduction='mean'):\n",
    "        nn.Module.__init__(self)\n",
    "        self.weight = weight\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, input_tensor, target_tensor):\n",
    "        log_prob = F.log_softmax(input_tensor, dim=-1)\n",
    "        prob = torch.exp(log_prob)\n",
    "        return F.nll_loss(\n",
    "            ((1 - prob) ** self.gamma) * log_prob,\n",
    "            target_tensor,\n",
    "            weight=self.weight,\n",
    "            reduction=self.reduction\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "filled-configuration",
   "metadata": {
    "id": "filled-configuration",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LabelSmoothingLoss(nn.Module):\n",
    "    def __init__(self, classes=3, smoothing=0.0, dim=-1):\n",
    "        super(LabelSmoothingLoss, self).__init__()\n",
    "        self.confidence = 1.0 - smoothing\n",
    "        self.smoothing = smoothing\n",
    "        self.cls = classes\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, pred, target):\n",
    "        pred = pred.log_softmax(dim=self.dim)\n",
    "        with torch.no_grad():\n",
    "            true_dist = torch.zeros_like(pred)\n",
    "            true_dist.fill_(self.smoothing / (self.cls - 1))\n",
    "            true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n",
    "        return torch.mean(torch.sum(-true_dist * pred, dim=self.dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "informative-banks",
   "metadata": {
    "id": "informative-banks",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# https://gist.github.com/SuperShinyEyes/dcc68a08ff8b615442e3bc6a9b55a354\n",
    "class F1Loss(nn.Module):\n",
    "    def __init__(self, classes=3, epsilon=1e-7):\n",
    "        super().__init__()\n",
    "        self.classes = classes\n",
    "        self.epsilon = epsilon\n",
    "    def forward(self, y_pred, y_true):\n",
    "        assert y_pred.ndim == 2\n",
    "        assert y_true.ndim == 1\n",
    "        y_true = F.one_hot(y_true, self.classes).to(torch.float32)\n",
    "        y_pred = F.softmax(y_pred, dim=1)\n",
    "\n",
    "        tp = (y_true * y_pred).sum(dim=0).to(torch.float32)\n",
    "        tn = ((1 - y_true) * (1 - y_pred)).sum(dim=0).to(torch.float32)\n",
    "        fp = ((1 - y_true) * y_pred).sum(dim=0).to(torch.float32)\n",
    "        fn = (y_true * (1 - y_pred)).sum(dim=0).to(torch.float32)\n",
    "\n",
    "        precision = tp / (tp + fp + self.epsilon)\n",
    "        recall = tp / (tp + fn + self.epsilon)\n",
    "\n",
    "        f1 = 2 * (precision * recall) / (precision + recall + self.epsilon)\n",
    "        f1 = f1.clamp(min=self.epsilon, max=1 - self.epsilon)\n",
    "        return 1 - f1.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "desirable-accounting",
   "metadata": {
    "id": "desirable-accounting",
    "tags": []
   },
   "outputs": [],
   "source": [
    "_criterion_entrypoints = {\n",
    "    'cross_entropy': nn.CrossEntropyLoss,\n",
    "    'focal': FocalLoss,\n",
    "    'label_smoothing': LabelSmoothingLoss,\n",
    "    'f1': F1Loss\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gothic-picking",
   "metadata": {
    "id": "gothic-picking"
   },
   "source": [
    "## 2. Optimizer\n",
    "- 파이토치는 코드를 간단히 수정하여 다양한 optimizer를 사용할 수 있습니다.\n",
    "- 이번 실습 자료에서는 Adam 과 SGD Optimizer 를 활용하는 방법을 알아봅니다.\n",
    "\n",
    "\n",
    "### 2.1 예제를 통해 그래프로 이해하기\n",
    "- 예제를 통해 Optimizer 따라 Loss 값이 변하는 추이를 그래프를 통해서 이해해봅시다.\n",
    "- 비교할 Optimizer 는 베이스라인 코드에서 default 로 사용된 SGD와 Adam Optimizer 입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c1313c",
   "metadata": {
    "id": "b9c1313c"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.data as Data\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "# -- 기본 모델 정의.\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.hidden = torch.nn.Linear(1, 20)   # 은닉층\n",
    "        self.predict = torch.nn.Linear(20, 1)   # 결과층\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.hidden(x))      # activation function for hidden layer\n",
    "        x = self.predict(x)             # 결과.\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f8b3f99",
   "metadata": {
    "id": "0f8b3f99"
   },
   "outputs": [],
   "source": [
    "# -- Hyperparameter 정의.\n",
    "LR = 0.01  #Learning rate\n",
    "BATCH_SIZE = 32 \n",
    "EPOCH = 15 \n",
    "\n",
    "x = torch.unsqueeze(torch.linspace(-1,1,1000),dim=1)\n",
    "y = x.pow(2) + 0.1*torch.normal(torch.zeros(x.size()))\n",
    "\n",
    "# -- 데이터셋 생성.\n",
    "torch_dataset = Data.TensorDataset(x,y)\n",
    "loader = Data.DataLoader(\n",
    "    dataset=torch_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d863901",
   "metadata": {
    "id": "1d863901"
   },
   "outputs": [],
   "source": [
    "# -- 각 optimizer를 사용한 신경망 정의.\n",
    "net_SGD = Net()\n",
    "net_Adam = Net()\n",
    "\n",
    "# -- 각 신경망 리스트.\n",
    "nets = [net_SGD, net_Adam]\n",
    "\n",
    "# -- 각 Optimizer 정의.\n",
    "opt_SGD = torch.optim.SGD(net_SGD.parameters(), lr=LR) # SGD Optimizer\n",
    "opt_Adam = torch.optim.Adam(net_Adam.parameters(), lr=LR, betas=(0.9,0.99)) # Adam Optimizer\n",
    "\n",
    "# -- 각 Optimizer 리스트.\n",
    "optimizers = [opt_SGD, opt_Adam]\n",
    "\n",
    "# -- Loss 계산.\n",
    "loss_func = torch.nn.MSELoss()\n",
    "# -- Loss 기록.\n",
    "losses_his = [[],[]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc5259c",
   "metadata": {
    "id": "8cc5259c"
   },
   "outputs": [],
   "source": [
    "#-- Training \n",
    "for epoch in range(EPOCH):\n",
    "    print(\"[INFO] Training Epoch : {}\".format(epoch))\n",
    "    for step, (batch_x,batch_y) in enumerate(loader):\n",
    "        \n",
    "        b_x = Variable(batch_x)\n",
    "        b_y = Variable(batch_y)\n",
    "                \n",
    "        for net, opt, l_his in zip(nets, optimizers, losses_his):\n",
    "            output = net(b_x) \n",
    "            loss = loss_func(output, b_y) #각 신경망의 Loss 계산.\n",
    "            \n",
    "            opt.zero_grad() \n",
    "            loss.backward() \n",
    "            opt.step() \n",
    "            \n",
    "\n",
    "            l_his.append(loss.item()) \n",
    "print(\"[INFO] Training ALL DONE!!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef0bbe5",
   "metadata": {
    "id": "0ef0bbe5"
   },
   "outputs": [],
   "source": [
    "#-- 그래프로 시각화.\n",
    "labels = ['SGD', 'Adam']\n",
    "colors = ['darkred', 'royalblue']\n",
    "for i, l_his in enumerate(losses_his):\n",
    "    plt.plot(l_his, label=labels[i], color=colors[i])\n",
    "plt.legend(loc='best')\n",
    "plt.xlabel('Steps')\n",
    "plt.ylabel('Loss')\n",
    "plt.ylim(0,0.25)  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb6e694b",
   "metadata": {
    "id": "bb6e694b"
   },
   "source": [
    "- 그래프를 통해 Adam 의 Loss 값이 SGD에서 더 낮은것을 확인할 수 있습니다! 실제로 베이스라인 코드에 적용해 볼까요?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f70ca3a",
   "metadata": {
    "id": "3f70ca3a"
   },
   "source": [
    "### 2.2 베이스라인 코드에 적용한 결과 살펴보기\n",
    "\n",
    "- 이번엔 베이스라인 코드에서 SGD Optimizer를 먼저 적용해 보고 Adam Optimizer를 적용한 결과의 Loss와 Accuracy의 변화를 비교해 살펴보겠습니다.\n",
    "- 결론 부터 말씀드리면 Adam(Adaptive moment estimation) Optimizer 는 다른 stochastic optimization methods(SGD, Adagrad, RMSProp)에 비해 optimizer 성능이 좋은 편입니다! 실제로 확인해볼까요? [참고](https://arxiv.org/pdf/1412.6980.pdf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b5571a7",
   "metadata": {
    "id": "2b5571a7"
   },
   "source": [
    "- 다음은 베이스라인 코드중 train.py 파일의 일부분 입니다. \n",
    "\n",
    "###### Libraries & Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec5887c5",
   "metadata": {
    "id": "ec5887c5"
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import glob\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "from importlib import import_module\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.utils.data import Subset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from dataset import MaskBaseDataset\n",
    "from loss import create_criterion\n",
    "\n",
    "\n",
    "def seed_everything(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  # if use multi-GPU\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "\n",
    "\n",
    "def get_lr(optimizer):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        return param_group['lr']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "340d2986",
   "metadata": {
    "id": "340d2986"
   },
   "source": [
    "###### Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4878c25",
   "metadata": {
    "id": "a4878c25"
   },
   "outputs": [],
   "source": [
    "# -- loss & metric\n",
    "criterion = create_criterion(args.criterion)  # default: cross_entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c831b4",
   "metadata": {
    "id": "05c831b4"
   },
   "source": [
    "- 아래쪽의 Optimizer를 적용하는 부분은 다음과 같이 간단히 한줄로 적용할 수도 있습니다.\n",
    "###### SGD optimizer\n",
    "    optimizer = SGD(model.parameters(), lr=lr, weight_decay=5e-4)\n",
    "###### Adam optimizer\n",
    "    optimizer = Adam(model.parameters(), lr=lr, weight_decay=5e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e008b57e",
   "metadata": {
    "id": "e008b57e"
   },
   "source": [
    "###### Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d21259b",
   "metadata": {
    "id": "9d21259b"
   },
   "outputs": [],
   "source": [
    "opt_module = getattr(import_module(\"torch.optim\"), args.optimizer)  # default: SGD\n",
    "optimizer = opt_module(\n",
    "    filter(lambda p: p.requires_grad, model.parameters()),\n",
    "    lr=args.lr,\n",
    "    weight_decay=5e-4\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "641da963",
   "metadata": {
    "id": "641da963"
   },
   "source": [
    "###### Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf2e49c",
   "metadata": {
    "id": "bcf2e49c"
   },
   "outputs": [],
   "source": [
    "scheduler = StepLR(optimizer, args.lr_decay_step, gamma=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f642b9d",
   "metadata": {
    "id": "5f642b9d"
   },
   "source": [
    "- SGD Optimizer 결과."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66195987",
   "metadata": {
    "id": "66195987"
   },
   "source": [
    "Epoch[98/100](20/241) || training loss 2.278 || training accuracy 26.56% || lr 6.25e-05\n",
    "\n",
    "Epoch[98/100](40/241) || training loss 2.261 || training accuracy 28.44% || lr 6.25e-05\n",
    "\n",
    "Epoch[98/100](60/241) || training loss 2.258 || training accuracy 28.75% || lr 6.25e-05\n",
    "\n",
    "Epoch[98/100](80/241) || training loss 2.286 || training accuracy 28.83% || lr 6.25e-05\n",
    "\n",
    "Epoch[98/100](100/241) || training loss  2.3 || training accuracy 28.05% || lr 6.25e-05\n",
    "\n",
    "Epoch[98/100](120/241) || training loss 2.298 || training accuracy 26.48% || lr 6.25e-05\n",
    "\n",
    "Epoch[98/100](140/241) || training loss 2.232 || training accuracy 28.75% || lr 6.25e-05\n",
    "\n",
    "Epoch[98/100](160/241) || training loss 2.338 || training accuracy 26.56% || lr 6.25e-05\n",
    "\n",
    "Epoch[98/100](180/241) || training loss 2.206 || training accuracy 29.61% || lr 6.25e-05\n",
    "\n",
    "Epoch[98/100](200/241) || training loss  2.3 || training accuracy 25.86% || lr 6.25e-05\n",
    "\n",
    "Epoch[98/100](220/241) || training loss 2.269 || training accuracy 27.89% || lr 6.25e-05\n",
    "\n",
    "Epoch[98/100](240/241) || training loss 2.294 || training accuracy 27.97% || lr 6.25e-05\n",
    "\n",
    "Calculating validation results...\n",
    "[Val] acc : 24.71%, loss:  2.3 || best acc : 24.80%, best loss:  2.3\n",
    "\n",
    "Epoch[99/100](20/241) || training loss 2.258 || training accuracy 28.98% || lr 6.25e-05\n",
    "\n",
    "Epoch[99/100](40/241) || training loss 2.273 || training accuracy 27.66% || lr 6.25e-05\n",
    "\n",
    "Epoch[99/100](60/241) || training loss 2.299 || training accuracy 25.47% || lr 6.25e-05\n",
    "\n",
    "Epoch[99/100](80/241) || training loss 2.325 || training accuracy 26.02% || lr 6.25e-05\n",
    "\n",
    "Epoch[99/100](100/241) || training loss 2.27 || training accuracy 26.25% || lr 6.25e-05\n",
    "\n",
    "Epoch[99/100](120/241) || training loss 2.237 || training accuracy 29.38% || lr 6.25e-05\n",
    "\n",
    "Epoch[99/100](140/241) || training loss 2.281 || training accuracy 27.89% || lr 6.25e-05\n",
    "\n",
    "Epoch[99/100](160/241) || training loss 2.264 || training accuracy 28.91% || lr 6.25e-05\n",
    "\n",
    "Epoch[99/100](180/241) || training loss 2.256 || training accuracy 30.16% || lr 6.25e-05\n",
    "\n",
    "Epoch[99/100](200/241) || training loss 2.304 || training accuracy 28.12% || lr 6.25e-05\n",
    "\n",
    "Epoch[99/100](220/241) || training loss 2.262 || training accuracy 29.38% || lr 6.25e-05\n",
    "\n",
    "Epoch[99/100](240/241) || training loss 2.282 || training accuracy 27.42% || lr 6.25e-05\n",
    "\n",
    "Calculating validation results...\n",
    "[Val] acc : 24.80%, loss:  2.3 || best acc : 24.80%, best loss:  2.3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9df4610",
   "metadata": {
    "id": "b9df4610"
   },
   "source": [
    "- Adam Optimizer 결과로, Optimizer 만 바꿨을 뿐인데 확연히 낮아진 Loss값과 향상된 Accuracy 를 확인할 수 있습니다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef82e7f0",
   "metadata": {
    "id": "ef82e7f0"
   },
   "source": [
    "Epoch[98/100](20/241) || training loss 0.3912 || training accuracy 86.72% || lr 6.25e-05\n",
    "\n",
    "Epoch[98/100](40/241) || training loss 0.4169 || training accuracy 86.72% || lr 6.25e-05\n",
    "\n",
    "Epoch[98/100](60/241) || training loss 0.4019 || training accuracy 86.56% || lr 6.25e-05\n",
    "\n",
    "Epoch[98/100](80/241) || training loss 0.428 || training accuracy 85.70% || lr 6.25e-05\n",
    "\n",
    "Epoch[98/100](100/241) || training loss 0.4092 || training accuracy 85.78% || lr 6.25e-05\n",
    "\n",
    "Epoch[98/100](120/241) || training loss 0.4237 || training accuracy 86.33% || lr 6.25e-05\n",
    "\n",
    "Epoch[98/100](140/241) || training loss 0.4093 || training accuracy 86.25% || lr 6.25e-05\n",
    "\n",
    "Epoch[98/100](160/241) || training loss 0.3903 || training accuracy 87.19% || lr 6.25e-05\n",
    "\n",
    "Epoch[98/100](180/241) || training loss 0.4056 || training accuracy 86.64% || lr 6.25e-05\n",
    "\n",
    "Epoch[98/100](200/241) || training loss 0.408 || training accuracy 86.72% || lr 6.25e-05\n",
    "\n",
    "Epoch[98/100](220/241) || training loss 0.4367 || training accuracy 84.92% || lr 6.25e-05\n",
    "\n",
    "Epoch[98/100](240/241) || training loss 0.4363 || training accuracy 85.55% || lr 6.25e-05\n",
    "\n",
    "Calculating validation results...\n",
    "[Val] acc : 68.12%, loss: 0.71 || best acc : 68.30%, best loss:  0.7\n",
    "\n",
    "Epoch[99/100](20/241) || training loss 0.3987 || training accuracy 87.11% || lr 6.25e-05\n",
    "\n",
    "Epoch[99/100](40/241) || training loss 0.4065 || training accuracy 87.89% || lr 6.25e-05\n",
    "\n",
    "Epoch[99/100](60/241) || training loss 0.4084 || training accuracy 86.25% || lr 6.25e-05\n",
    "\n",
    "Epoch[99/100](80/241) || training loss 0.4135 || training accuracy 86.56% || lr 6.25e-05\n",
    "\n",
    "Epoch[99/100](100/241) || training loss 0.4168 || training accuracy 85.78% || lr 6.25e-05\n",
    "\n",
    "Epoch[99/100](120/241) || training loss 0.3895 || training accuracy 86.48% || lr 6.25e-05\n",
    "\n",
    "Epoch[99/100](140/241) || training loss 0.4228 || training accuracy 85.62% || lr 6.25e-05\n",
    "\n",
    "Epoch[99/100](160/241) || training loss 0.4314 || training accuracy 85.86% || lr 6.25e-05\n",
    "\n",
    "Epoch[99/100](180/241) || training loss 0.4434 || training accuracy 84.92% || lr 6.25e-05\n",
    "\n",
    "Epoch[99/100](200/241) || training loss 0.4697 || training accuracy 84.53% || lr 6.25e-05\n",
    "\n",
    "Epoch[99/100](220/241) || training loss 0.3847 || training accuracy 87.73% || lr 6.25e-05\n",
    "\n",
    "Epoch[99/100](240/241) || training loss 0.4097 || training accuracy 85.86% || lr 6.25e-05\n",
    "\n",
    "Calculating validation results...\n",
    "[Val] acc : 67.80%, loss:  0.7 || best acc : 68.30%, best loss:  0.7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "informative-merchandise",
   "metadata": {
    "id": "informative-merchandise"
   },
   "source": [
    "## 3. Scheduler\n",
    "- Scheduler은 optimizer의 learning rate를 동적으로 변경시키는 기능을 합니다.\n",
    "- Optimizer과 Scheduler를 적절히 활용하면 모델이 좋은 성능으로 Fitting하는데 도움을 줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "original-breath",
   "metadata": {
    "id": "original-breath"
   },
   "outputs": [],
   "source": [
    "# -- scheduler: StepLR\n",
    "# 지정된 step마다 learning rate를 감소시킵니다.\n",
    "scheduler = StepLR(optimizer, lr_decay_step, gamma=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "resistant-compiler",
   "metadata": {
    "id": "resistant-compiler"
   },
   "outputs": [],
   "source": [
    "# -- scheduler: ReduceLROnPlateau\n",
    "# 성능이 향상되지 않을 때 learning rate를 줄입니다. patience=10은 10회 동안 성능 향상이 없을 경우입니다.\n",
    "scheduler = ReduceLROnPlateau(optimizer, factor=0.1, patience=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "distant-crowd",
   "metadata": {
    "id": "distant-crowd"
   },
   "outputs": [],
   "source": [
    "# -- scheduler: CosineAnnealingLR\n",
    "# CosineAnnealing은 learning rate를 cosine 그래프처럼 변화시킵니다.\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=2, eta_min=0.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "under-documentary",
   "metadata": {
    "id": "under-documentary"
   },
   "source": [
    "## 4. Metric\n",
    "- Classification 성능을 표현할 때 다양한 평가지표가 있습니다.\n",
    "- Accuracy: 모델이 정확하게 예측한 객체의 비율\n",
    "- True Positive(TP): 실제 True인 정답을 True라고 예측 (정답)\n",
    "- False Positive(FP): 실제 False인 정답을 True라고 예측 (오답)\n",
    "- False Negative(FN): 실제 True인 정답을 False라고 예측 (오답)\n",
    "- True Negative(TN): 실제 False인 정답을 False라고 예측 (정답)\n",
    "- Precision(정밀도): TP / (TP + FP)\n",
    "- Recall(재현율): TP / (TP + FN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "racial-zealand",
   "metadata": {
    "id": "racial-zealand"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "y_true = [0, 1, 2, 0, 1, 2]\n",
    "y_pred = [0, 2, 1, 0, 0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "union-laundry",
   "metadata": {
    "id": "union-laundry"
   },
   "outputs": [],
   "source": [
    "# -- Accuracy\n",
    "accuracy_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rotary-working",
   "metadata": {
    "id": "rotary-working"
   },
   "outputs": [],
   "source": [
    "# -- Accuracy\n",
    "# Normalize를 안하면 맞춘 개수가 표시된다\n",
    "accuracy_score(y_true, y_pred, normalize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "employed-torture",
   "metadata": {
    "id": "employed-torture"
   },
   "outputs": [],
   "source": [
    "# -- Precision\n",
    "precision = precision_score(y_true, y_pred, average='macro')\n",
    "precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "designed-storage",
   "metadata": {
    "id": "designed-storage"
   },
   "outputs": [],
   "source": [
    "# -- Recall\n",
    "recall = recall_score(y_true, y_pred, average='macro')\n",
    "recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "informed-consolidation",
   "metadata": {
    "id": "informed-consolidation"
   },
   "outputs": [],
   "source": [
    "# -- f1 score\n",
    "2 * (precision * recall) / (precision + recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "owned-victory",
   "metadata": {
    "id": "owned-victory"
   },
   "outputs": [],
   "source": [
    "# -- f1 score (sklearn)\n",
    "f1_score(y_true, y_pred, average='macro')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "foreign-guess",
   "metadata": {
    "id": "foreign-guess"
   },
   "source": [
    "## 5. Training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "several-principal",
   "metadata": {
    "id": "several-principal"
   },
   "outputs": [],
   "source": [
    "dataset = MaskMultiClassDataset(img_root, label_path, 'train')\n",
    "n_val = int(len(dataset) * val_split)\n",
    "n_train = len(dataset) - n_val\n",
    "train_set, val_set = torch.utils.data.random_split(dataset, [n_train, n_val])\n",
    "val_set.dataset.set_phase(\"test\")  # todo : fix\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_set,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=num_workers,\n",
    "    drop_last=True,\n",
    ")\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_set,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=num_workers,\n",
    "    drop_last=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bulgarian-moisture",
   "metadata": {
    "id": "bulgarian-moisture"
   },
   "source": [
    "### 5.1 Callback - Checkpoint, Early Stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "insured-craft",
   "metadata": {
    "id": "insured-craft"
   },
   "outputs": [],
   "source": [
    "# -- Callback1: Checkpoint - Accuracy가 높아질 때마다 모델을 저장합니다.\n",
    "# 학습 코드에서 이어집니다.\n",
    "\n",
    "# -- Callback2: Early Stopping - 성능이 일정 기간동안 향상이 없을 경우 학습을 종료합니다.\n",
    "patience = 10\n",
    "counter = 0\n",
    "# 학습 코드에서 이어집니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "turned-citizenship",
   "metadata": {
    "id": "turned-citizenship"
   },
   "source": [
    "### 5.2 Training Method - Gradient Accumulation\n",
    "- Graident Accumulation은 한 iteration에 파라미터를 업데이트시키는게 아니라, gradient를 여러 iteration 동안 쌓아서 업데이트시킵니다. 한 번에 파라미터를 업데이트시키는 건 noise가 있을 수 있으므로, 여러번 쌓아서 한번에 업데이트 시킴으로써 그러한 문제를 방지하기 위함입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bibliographic-recycling",
   "metadata": {
    "id": "bibliographic-recycling"
   },
   "outputs": [],
   "source": [
    "# -- Gradient Accumulation\n",
    "accumulation_steps = 2\n",
    "# 학습코드에서 이어집니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "premium-poker",
   "metadata": {
    "id": "premium-poker"
   },
   "source": [
    "### 5.3 Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b612c2",
   "metadata": {
    "id": "24b612c2"
   },
   "outputs": [],
   "source": [
    "os.makedirs(os.path.join(os.getcwd(), 'results', name), exist_ok=True)\n",
    "\n",
    "counter = 0\n",
    "best_val_acc = 0\n",
    "best_val_loss = np.inf\n",
    "for epoch in range(num_epochs):\n",
    "    # train loop\n",
    "    model.train()\n",
    "    loss_value = 0\n",
    "    matches = 0\n",
    "    for idx, train_batch in enumerate(train_loader):\n",
    "        inputs, labels = train_batch\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outs = model(inputs)\n",
    "        preds = torch.argmax(outs, dim=-1)\n",
    "        loss = criterion(outs, labels)\n",
    "\n",
    "        loss.backward()\n",
    "        \n",
    "        # -- Gradient Accumulation\n",
    "        if (idx+1) % accumulation_steps == 0:\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        loss_value += loss.item()\n",
    "        matches += (preds == labels).sum().item()\n",
    "        if (idx + 1) % train_log_interval == 0:\n",
    "            train_loss = loss_value / train_log_interval\n",
    "            train_acc = matches / batch_size / train_log_interval\n",
    "            current_lr = scheduler.get_last_lr()\n",
    "            print(\n",
    "                f\"Epoch[{epoch}/{num_epochs}]({idx + 1}/{len(train_loader)}) || \"\n",
    "                f\"training loss {train_loss:4.4} || training accuracy {train_acc:4.2%} || lr {current_lr}\"\n",
    "            )\n",
    "\n",
    "            loss_value = 0\n",
    "            matches = 0\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    # val loop\n",
    "    with torch.no_grad():\n",
    "        print(\"Calculating validation results...\")\n",
    "        model.eval()\n",
    "        val_loss_items = []\n",
    "        val_acc_items = []\n",
    "        for val_batch in val_loader:\n",
    "            inputs, labels = val_batch\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outs = model(inputs)\n",
    "            preds = torch.argmax(outs, dim=-1)\n",
    "\n",
    "            loss_item = criterion(outs, labels).item()\n",
    "            acc_item = (labels == preds).sum().item()\n",
    "            val_loss_items.append(loss_item)\n",
    "            val_acc_items.append(acc_item)\n",
    "\n",
    "        val_loss = np.sum(val_loss_items) / len(val_loader)\n",
    "        val_acc = np.sum(val_acc_items) / len(val_set)\n",
    "        \n",
    "        # Callback1: validation accuracy가 향상될수록 모델을 저장합니다.\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "        if val_acc > best_val_acc:\n",
    "            print(\"New best model for val accuracy! saving the model..\")\n",
    "            torch.save(model.state_dict(), f\"results/{name}/{epoch:03}_accuracy_{val_acc:4.2%}.ckpt\")\n",
    "            best_val_acc = val_acc\n",
    "            counter = 0\n",
    "        else:\n",
    "            counter += 1\n",
    "        # Callback2: patience 횟수 동안 성능 향상이 없을 경우 학습을 종료시킵니다.\n",
    "        if counter > patience:\n",
    "            print(\"Early Stopping...\")\n",
    "            break\n",
    "        \n",
    "        \n",
    "        print(\n",
    "            f\"[Val] acc : {val_acc:4.2%}, loss: {val_loss:4.2} || \"\n",
    "            f\"best acc : {best_val_acc:4.2%}, best loss: {best_val_loss:4.2}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "xjFvO2P-cSlv",
   "metadata": {
    "id": "xjFvO2P-cSlv"
   },
   "source": [
    "###**콘텐츠 라이선스**\n",
    "\n",
    "<font color='red'><b>**WARNING**</b></font> : **본 교육 콘텐츠의 지식재산권은 재단법인 네이버커넥트에 귀속됩니다. 본 콘텐츠를 어떠한 경로로든 외부로 유출 및 수정하는 행위를 엄격히 금합니다.** 다만, 비영리적 교육 및 연구활동에 한정되어 사용할 수 있으나 재단의 허락을 받아야 합니다. 이를 위반하는 경우, 관련 법률에 따라 책임을 질 수 있습니다."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
