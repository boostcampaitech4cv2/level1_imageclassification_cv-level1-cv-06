{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import *\n",
    "from torch.optim.lr_scheduler import *\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import random\n",
    "import time\n",
    "from datetime import timedelta\n",
    "import shutil\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from pytorch_model_summary import summary\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import multiprocessing\n",
    "import timm\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import os\n",
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./dataset/train/hypothesis_valid.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3780"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mask ensemble effect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 17 5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "/opt/ml/repo/level1_imageclassification_cv-level1-cv-06/T4064/dataset/train/images/006511_male_Asian_19/mask3.jpg               3\n",
       "/opt/ml/repo/level1_imageclassification_cv-level1-cv-06/T4064/dataset/train/images/006269_female_Asian_20/incorrect_mask.jpg    2\n",
       "/opt/ml/repo/level1_imageclassification_cv-level1-cv-06/T4064/dataset/train/images/006006_female_Asian_18/mask5.jpg             2\n",
       "/opt/ml/repo/level1_imageclassification_cv-level1-cv-06/T4064/dataset/train/images/006397_male_Asian_19/mask5.jpg               2\n",
       "/opt/ml/repo/level1_imageclassification_cv-level1-cv-06/T4064/dataset/train/images/005536_female_Asian_39/normal.jpg            1\n",
       "/opt/ml/repo/level1_imageclassification_cv-level1-cv-06/T4064/dataset/train/images/005473_female_Asian_51/mask2.jpg             1\n",
       "/opt/ml/repo/level1_imageclassification_cv-level1-cv-06/T4064/dataset/train/images/005459_male_Asian_60/incorrect_mask.jpg      1\n",
       "/opt/ml/repo/level1_imageclassification_cv-level1-cv-06/T4064/dataset/train/images/006098_male_Asian_18/incorrect_mask.jpg      1\n",
       "/opt/ml/repo/level1_imageclassification_cv-level1-cv-06/T4064/dataset/train/images/005448_female_Asian_54/mask3.jpg             1\n",
       "/opt/ml/repo/level1_imageclassification_cv-level1-cv-06/T4064/dataset/train/images/005464_male_Asian_47/mask2.jpg               1\n",
       "/opt/ml/repo/level1_imageclassification_cv-level1-cv-06/T4064/dataset/train/images/006473_female_Asian_18/mask4.jpg             1\n",
       "/opt/ml/repo/level1_imageclassification_cv-level1-cv-06/T4064/dataset/train/images/006571_female_Asian_21/mask5.jpg             1\n",
       "/opt/ml/repo/level1_imageclassification_cv-level1-cv-06/T4064/dataset/train/images/006267_female_Asian_20/incorrect_mask.jpg    1\n",
       "/opt/ml/repo/level1_imageclassification_cv-level1-cv-06/T4064/dataset/train/images/006056_female_Asian_19/mask5.png             1\n",
       "/opt/ml/repo/level1_imageclassification_cv-level1-cv-06/T4064/dataset/train/images/006384_male_Asian_19/mask2.jpg               1\n",
       "/opt/ml/repo/level1_imageclassification_cv-level1-cv-06/T4064/dataset/train/images/006905_male_Asian_19/mask4.jpg               1\n",
       "/opt/ml/repo/level1_imageclassification_cv-level1-cv-06/T4064/dataset/train/images/006380_male_Asian_19/mask3.jpg               1\n",
       "/opt/ml/repo/level1_imageclassification_cv-level1-cv-06/T4064/dataset/train/images/006708_male_Asian_19/incorrect_mask.jpg      1\n",
       "/opt/ml/repo/level1_imageclassification_cv-level1-cv-06/T4064/dataset/train/images/006057_female_Asian_19/incorrect_mask.png    1\n",
       "/opt/ml/repo/level1_imageclassification_cv-level1-cv-06/T4064/dataset/train/images/006424_female_Asian_18/mask4.jpg             1\n",
       "/opt/ml/repo/level1_imageclassification_cv-level1-cv-06/T4064/dataset/train/images/006367_male_Asian_19/mask2.jpg               1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask1_diff = pd.read_csv('./checkpoints/mask1/diff.csv')\n",
    "mask2_diff = pd.read_csv('./checkpoints/mask2/diff.csv')\n",
    "mask3_diff = pd.read_csv('./checkpoints/mask3/diff.csv')\n",
    "mask_error_img1 = df[mask1_diff['labels']!=mask1_diff['preds']]['path'].tolist()\n",
    "mask_error_img2 = df[mask2_diff['labels']!=mask2_diff['preds']]['path'].tolist()\n",
    "mask_error_img3 = df[mask3_diff['labels']!=mask3_diff['preds']]['path'].tolist()\n",
    "print(len(mask_error_img1), len(mask_error_img2), len(mask_error_img3))\n",
    "total_error_case = []\n",
    "total_error_case.extend(mask_error_img1)\n",
    "total_error_case.extend(mask_error_img2)\n",
    "total_error_case.extend(mask_error_img3)\n",
    "pd.Series(total_error_case).value_counts() # 총 4개가 틀리개 됨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### gender ensemble effect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105 118 122\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False    103\n",
       "True      92\n",
       "dtype: int64"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gender_diff1 = pd.read_csv('./checkpoints/gender_cls1/diff.csv')\n",
    "gender_diff2 = pd.read_csv('./checkpoints/gender_cls2/diff.csv')\n",
    "gender_diff3 = pd.read_csv('./checkpoints/gender_cls3/diff.csv')\n",
    "gender_error_img1 = df[gender_diff1['labels']!=gender_diff1['preds']]['path'].tolist()\n",
    "gender_error_img2 = df[gender_diff2['labels']!=gender_diff2['preds']]['path'].tolist()\n",
    "gender_error_img3 = df[gender_diff3['labels']!=gender_diff3['preds']]['path'].tolist()\n",
    "print(len(gender_error_img1), len(gender_error_img2), len(gender_error_img3))\n",
    "total_error_case = []\n",
    "total_error_case.extend(gender_error_img1)\n",
    "total_error_case.extend(gender_error_img2)\n",
    "total_error_case.extend(gender_error_img3)\n",
    "bool_tag = pd.Series(total_error_case).value_counts()>=2 # 총 4개가 틀리개 됨\n",
    "bool_tag.value_counts() # 총 92개 틀리게 됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
